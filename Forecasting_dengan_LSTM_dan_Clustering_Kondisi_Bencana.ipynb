{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forecasting dengan LSTM dan Clustering Kondisi Bencana.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9t1PvH2OmmrT",
        "0P-dlxw13dGm",
        "0uZXHlZW965W",
        "BMgu54Bv-wo3",
        "PIefHK8pBPdv",
        "uQx5x8G9Ffny",
        "SCjG9wtyFip-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ZitVzU_jfvMR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas import concat\n",
        "from pandas import DataFrame\n",
        "numpy.random.seed(44)\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Suhu RH"
      ],
      "metadata": {
        "id": "9t1PvH2OmmrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset_suhu_&_kelembapan.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "JaP2H8sof_pi"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpqBq0rSgWd5",
        "outputId": "a3eb20e1-ead9-4e0b-d6ff-d987abb5d8ac"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8036 entries, 0 to 8035\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Tavg    7859 non-null   float64\n",
            " 1   RH_avg  7855 non-null   float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 125.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(len(df)*.6)\n",
        "dfTrain = df.iloc[:split, :]\n",
        "dfTest = df.iloc[split:, :]\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dfTrain = scaler.fit_transform(dfTrain)\n",
        "dfTest = scaler.transform(dfTest)"
      ],
      "metadata": {
        "id": "0PNQvCWknVww"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, 'scalersuhurh.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZVbpVD2DEJD",
        "outputId": "69acd0b5-9b30-43c3-af95-2684725f348f"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scalersuhurh.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deret_waktu(data, n_in=1, n_out=1, dropnan=True,):\n",
        "    n_vars = (1 if type(data) is list else data.shape[1])\n",
        "    df = DataFrame(data)\n",
        "    (cols, names) = (list(), list())\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += ['var%d(t-%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += ['var%d(t)' % (j + 1) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += ['var%d(t+%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "3762D-A6h0cm"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag = 2\n",
        "output = 1\n",
        "reframedTrain = deret_waktu(dfTrain, lag, output)\n",
        "reframedTest = deret_waktu(dfTest, lag, output)"
      ],
      "metadata": {
        "id": "NQ9KzoWshaqe"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframedTest.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rqx_A_KmnG9u",
        "outputId": "f64e4e0e-783b-43d6-9f2e-554bb5130b41"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   var1(t-2)  var2(t-2)  var1(t-1)  var2(t-1)   var1(t)   var2(t)\n",
              "2   0.353659   0.686275   0.451220   0.627451  0.451220  0.549020\n",
              "3   0.451220   0.627451   0.451220   0.549020  0.451220  0.549020\n",
              "4   0.451220   0.549020   0.451220   0.549020  0.402439  0.470588\n",
              "5   0.451220   0.549020   0.402439   0.470588  0.402439  0.549020\n",
              "6   0.402439   0.470588   0.402439   0.549020  0.475610  0.568627"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e042eca-ab29-42f0-953c-b958861c4cb1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-2)</th>\n",
              "      <th>var2(t-2)</th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var2(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "      <th>var2(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.353659</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.451220</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.549020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.475610</td>\n",
              "      <td>0.568627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e042eca-ab29-42f0-953c-b958861c4cb1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e042eca-ab29-42f0-953c-b958861c4cb1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e042eca-ab29-42f0-953c-b958861c4cb1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YTrain = reframedTrain.iloc[:, -2:].values\n",
        "YTest = reframedTest.iloc[:, -2:].values\n",
        "n_features = len(df.columns)\n",
        "n_obs = lag * n_features\n",
        "train_X, train_y = reframedTrain.iloc[:, :n_obs].values, YTrain\n",
        "test_X, test_y = reframedTest.iloc[:, :n_obs].values, YTest\n",
        "train_X = train_X.reshape((train_X.shape[0], lag, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], lag, n_features))"
      ],
      "metadata": {
        "id": "iCQSpc7kspo1"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(train_X.shape[-2:])))\n",
        "model.add(Dense(2, activation=\"relu\"))\n",
        "model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              loss='mse')  \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFPLlZKLxTQu",
        "outputId": "f9b3645f-afb9-4f2b-9004-03c063f1eb20"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_5 (LSTM)               (None, 256)               265216    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 265,730\n",
            "Trainable params: 265,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'suhurh.h5'\n",
        "modelckpt_callback = tensorflow.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
        "                                                     filepath=path, \n",
        "                                                     verbose=1, \n",
        "                                                     save_weights_only=False, \n",
        "                                                     save_best_only=True)\n",
        "es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                            min_delta=0, patience=10)\n",
        "history = model.fit(train_X, train_y, epochs=500, batch_size=8, \n",
        "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
        "                    callbacks=[modelckpt_callback, es_callback])\n",
        "\n",
        "loss = history.history['loss']\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lywnvZgpxXiE",
        "outputId": "1ae351f9-19a2-4a22-fcbb-3e4dfdceb558"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.00792, saving model to model_checkpoint.h5\n",
            "761/761 - 18s - loss: 0.0159 - val_loss: 0.0079 - 18s/epoch - 24ms/step\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 2: val_loss improved from 0.00792 to 0.00763, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0099 - val_loss: 0.0076 - 3s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 3: val_loss improved from 0.00763 to 0.00749, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0097 - val_loss: 0.0075 - 3s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 4: val_loss improved from 0.00749 to 0.00745, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0096 - val_loss: 0.0074 - 3s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 5: val_loss improved from 0.00745 to 0.00738, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0096 - val_loss: 0.0074 - 3s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 6: val_loss improved from 0.00738 to 0.00730, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0095 - val_loss: 0.0073 - 3s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: val_loss improved from 0.00730 to 0.00718, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0094 - val_loss: 0.0072 - 3s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 8: val_loss improved from 0.00718 to 0.00708, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0092 - val_loss: 0.0071 - 3s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 9: val_loss improved from 0.00708 to 0.00701, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0091 - val_loss: 0.0070 - 3s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 10: val_loss improved from 0.00701 to 0.00696, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0090 - val_loss: 0.0070 - 3s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 11: val_loss improved from 0.00696 to 0.00692, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0089 - val_loss: 0.0069 - 3s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 12: val_loss improved from 0.00692 to 0.00689, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0089 - val_loss: 0.0069 - 3s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 13: val_loss improved from 0.00689 to 0.00686, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0088 - val_loss: 0.0069 - 3s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 14: val_loss improved from 0.00686 to 0.00684, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0088 - val_loss: 0.0068 - 3s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 15: val_loss improved from 0.00684 to 0.00681, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0088 - val_loss: 0.0068 - 3s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 16: val_loss improved from 0.00681 to 0.00678, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0087 - val_loss: 0.0068 - 3s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 17: val_loss improved from 0.00678 to 0.00676, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0087 - val_loss: 0.0068 - 3s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: val_loss improved from 0.00676 to 0.00674, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0087 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: val_loss improved from 0.00674 to 0.00672, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0086 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 20: val_loss improved from 0.00672 to 0.00671, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0086 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 21: val_loss improved from 0.00671 to 0.00669, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0086 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 22: val_loss improved from 0.00669 to 0.00668, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0085 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 23: val_loss improved from 0.00668 to 0.00667, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0085 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 24: val_loss improved from 0.00667 to 0.00665, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0085 - val_loss: 0.0067 - 3s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 25: val_loss improved from 0.00665 to 0.00664, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0085 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 26: val_loss improved from 0.00664 to 0.00663, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0085 - val_loss: 0.0066 - 4s/epoch - 5ms/step\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 27: val_loss improved from 0.00663 to 0.00662, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0085 - val_loss: 0.0066 - 4s/epoch - 6ms/step\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 28: val_loss improved from 0.00662 to 0.00661, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0084 - val_loss: 0.0066 - 5s/epoch - 6ms/step\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 29: val_loss improved from 0.00661 to 0.00661, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0084 - val_loss: 0.0066 - 5s/epoch - 6ms/step\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 30: val_loss improved from 0.00661 to 0.00660, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0084 - val_loss: 0.0066 - 4s/epoch - 6ms/step\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 31: val_loss improved from 0.00660 to 0.00659, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0084 - val_loss: 0.0066 - 5s/epoch - 7ms/step\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 32: val_loss improved from 0.00659 to 0.00659, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 33: val_loss improved from 0.00659 to 0.00658, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 34: val_loss improved from 0.00658 to 0.00658, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 35: val_loss improved from 0.00658 to 0.00658, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 36: val_loss improved from 0.00658 to 0.00657, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 37: val_loss improved from 0.00657 to 0.00657, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0084 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 38: val_loss improved from 0.00657 to 0.00656, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 3ms/step\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 39: val_loss improved from 0.00656 to 0.00656, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 40: val_loss improved from 0.00656 to 0.00656, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 41: val_loss improved from 0.00656 to 0.00656, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 3ms/step\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 42: val_loss improved from 0.00656 to 0.00655, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 3ms/step\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 43: val_loss improved from 0.00655 to 0.00655, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0066 - 3s/epoch - 3ms/step\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 44: val_loss improved from 0.00655 to 0.00655, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 45: val_loss improved from 0.00655 to 0.00655, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 46: val_loss improved from 0.00655 to 0.00654, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 47: val_loss improved from 0.00654 to 0.00654, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 48: val_loss improved from 0.00654 to 0.00654, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 49: val_loss improved from 0.00654 to 0.00654, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 50: val_loss improved from 0.00654 to 0.00654, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 51: val_loss improved from 0.00654 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 52: val_loss improved from 0.00653 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 53: val_loss improved from 0.00653 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 54: val_loss improved from 0.00653 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 55: val_loss improved from 0.00653 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 56: val_loss improved from 0.00653 to 0.00653, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 57: val_loss improved from 0.00653 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 58: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 59: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0083 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 60: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 61: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 62: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 63: val_loss improved from 0.00652 to 0.00652, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 64: val_loss improved from 0.00652 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 65: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 66: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 67: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 68: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 69: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 70: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 71: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 72: val_loss improved from 0.00651 to 0.00651, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 73: val_loss improved from 0.00651 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 74: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 75: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 76: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 77: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 78: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 79: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0082 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 80: val_loss improved from 0.00650 to 0.00650, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0082 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 81: val_loss improved from 0.00650 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 82: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 83: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 84: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 85: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 86: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 87: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 88: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 89: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 90: val_loss improved from 0.00649 to 0.00649, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0082 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 91: val_loss improved from 0.00649 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 92: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 93: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 94: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 95: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 96: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 97: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 98: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 99: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 100: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 101: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 102: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 103: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 104: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 105: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 106: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 107: val_loss improved from 0.00648 to 0.00648, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 108: val_loss improved from 0.00648 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 109: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0082 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 110: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 111: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0082 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 112: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 113: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 114: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 115: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 116: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 117: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 118: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 119: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 120: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 121: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 122: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 123: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 124: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 125: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 126: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 127: val_loss improved from 0.00647 to 0.00647, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 128: val_loss improved from 0.00647 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 129: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 130: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 131: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 132: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 133: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 134: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 135: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 136: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0065 - 5s/epoch - 7ms/step\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 137: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0065 - 5s/epoch - 6ms/step\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 138: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 139: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 140: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 141: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 5ms/step\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 142: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 143: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 144: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 145: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 146: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 147: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 148: val_loss improved from 0.00646 to 0.00646, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0065 - 4s/epoch - 6ms/step\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 149: val_loss improved from 0.00646 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 150: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 151: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 152: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 153: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 154: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 155: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 156: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 157: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 3ms/step\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 158: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 159: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0065 - 3s/epoch - 4ms/step\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 160: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 161: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 162: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 163: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 164: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 165: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 166: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 167: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 168: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 169: val_loss improved from 0.00645 to 0.00645, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 170: val_loss improved from 0.00645 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 171: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 172: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 173: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 174: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 175: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 176: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 177: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 5ms/step\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 178: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 179: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 180: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 181: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 182: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 183: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 184: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 185: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 186: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 187: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 188: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 189: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 190: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 191: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 192: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 193: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 194: val_loss improved from 0.00644 to 0.00644, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 195: val_loss improved from 0.00644 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 196: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 197: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 198: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 199: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 200: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 201: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 202: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 203: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 204: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 205: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 206: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 207: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 208: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 209: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 210: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 211: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 212: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 213: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 214: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 215: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 5ms/step\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 216: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 217: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 218: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 219: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 220: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 221: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 222: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 223: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 224: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 225: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 226: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 227: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 228: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 229: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 230: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 231: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 232: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 233: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 234: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 235: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 236: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 237: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 5ms/step\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 238: val_loss improved from 0.00643 to 0.00643, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 239: val_loss improved from 0.00643 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 240: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 241: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 242: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 243: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0081 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 244: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 7ms/step\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 245: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 6ms/step\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 246: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 5s - loss: 0.0081 - val_loss: 0.0064 - 5s/epoch - 7ms/step\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 247: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0081 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 248: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 249: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 250: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 251: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 252: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 253: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 254: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 255: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 256: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 257: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 258: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 259: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 260: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 261: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 262: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 263: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 264: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 265: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 266: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 267: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 268: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 269: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 270: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0080 - val_loss: 0.0064 - 4s/epoch - 6ms/step\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 271: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 4s - loss: 0.0080 - val_loss: 0.0064 - 4s/epoch - 5ms/step\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 272: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 273: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 274: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 275: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 276: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 277: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 278: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 279: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 280: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 281: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 282: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 283: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 284: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 285: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 286: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 287: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 288: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 289: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 290: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 291: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 292: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 293: val_loss improved from 0.00642 to 0.00642, saving model to model_checkpoint.h5\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 294: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 295: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 296: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 297: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 298: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 3ms/step\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 299: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 300: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 301: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 302: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 303: val_loss did not improve from 0.00642\n",
            "761/761 - 3s - loss: 0.0080 - val_loss: 0.0064 - 3s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(path)"
      ],
      "metadata": {
        "id": "-6Iu8toRx9yf"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_X)\n",
        "pred = scaler.inverse_transform(pred)\n",
        "real = scaler.inverse_transform(YTest)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYfbs3Gy0LpF",
        "outputId": "ebb5e90a-93e4-468b-ed76-d8dd5bc2a15f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[28.392149, 77.87124 ],\n",
              "       [28.39337 , 74.63616 ],\n",
              "       [28.383959, 73.60746 ],\n",
              "       ...,\n",
              "       [28.610823, 79.72526 ],\n",
              "       [28.763346, 80.11984 ],\n",
              "       [28.327421, 81.36441 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE Suhu: {}'.format(numpy.sqrt(numpy.mean((pred[:, 0]-real[:, 0])**2))))\n",
        "print('RMSE RH: {}'.format(numpy.sqrt(numpy.mean((pred[:, 1]-real[:, 1])**2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Dtcb1r0Tgu",
        "outputId": "019aabb0-358b-4672-bcc9-d33d43e36a0b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Suhu: 0.7174402945970367\n",
            "RMSE RH: 3.674201627469388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CO2"
      ],
      "metadata": {
        "id": "0P-dlxw13dGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset_co2.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "PBcflbrF3dGn"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5aebbb-26df-477d-bdae-1020a66482b3",
        "id": "n1XLofcA3dGo"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125 entries, 0 to 124\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   CO2     125 non-null    float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 1.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(len(df)*.6)\n",
        "dfTrain = df.iloc[:split, :]\n",
        "dfTest = df.iloc[split:, :]"
      ],
      "metadata": {
        "id": "i_Hz-fR73dGq"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deret_waktu(data, n_in=1, n_out=1, dropnan=True,):\n",
        "    n_vars = (1 if type(data) is list else data.shape[1])\n",
        "    df = DataFrame(data)\n",
        "    (cols, names) = (list(), list())\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += ['var%d(t-%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += ['var%d(t)' % (j + 1) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += ['var%d(t+%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "1EqOoaDm3dGr"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag = 2\n",
        "output = 1\n",
        "reframedTrain = deret_waktu(dfTrain, lag, output)\n",
        "reframedTest = deret_waktu(dfTest, lag, output)"
      ],
      "metadata": {
        "id": "_8vKV2dB3dGt"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframedTest.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "42835523-fa70-4595-aa9f-33966c18ea96",
        "id": "ZJQsra6C3dGu"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    var1(t-2)  var1(t-1)  var1(t)\n",
              "77     78.077     83.190   88.150\n",
              "78     83.190     88.150   96.579\n",
              "79     88.150     96.579   97.246\n",
              "80     96.579     97.246   84.463\n",
              "81     97.246     84.463   86.071"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47405d0f-c88c-45b0-80cd-85260af49e34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-2)</th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>78.077</td>\n",
              "      <td>83.190</td>\n",
              "      <td>88.150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>83.190</td>\n",
              "      <td>88.150</td>\n",
              "      <td>96.579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>88.150</td>\n",
              "      <td>96.579</td>\n",
              "      <td>97.246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>96.579</td>\n",
              "      <td>97.246</td>\n",
              "      <td>84.463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>97.246</td>\n",
              "      <td>84.463</td>\n",
              "      <td>86.071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47405d0f-c88c-45b0-80cd-85260af49e34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47405d0f-c88c-45b0-80cd-85260af49e34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47405d0f-c88c-45b0-80cd-85260af49e34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = len(df.columns)\n",
        "YTrain = reframedTrain.iloc[:, -n_features:].values\n",
        "YTest = reframedTest.iloc[:, -n_features:].values\n",
        "n_obs = lag * n_features\n",
        "train_X, train_y = reframedTrain.iloc[:, :n_obs].values, YTrain\n",
        "test_X, test_y = reframedTest.iloc[:, :n_obs].values, YTest\n",
        "train_X = train_X.reshape((train_X.shape[0], lag, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], lag, n_features))"
      ],
      "metadata": {
        "id": "E1e_42e73dGv"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(train_X.shape[-2:])))\n",
        "model.add(Dense(n_features, activation=\"relu\"))\n",
        "model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              loss='mse')  \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915e2d9c-dfc6-44c1-b414-d1fdef8a2a38",
        "id": "NlgxjJJH3dGw"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_18 (LSTM)              (None, 256)               264192    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 264,449\n",
            "Trainable params: 264,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'co2.h5'\n",
        "modelckpt_callback = tensorflow.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
        "                                                     filepath=path, \n",
        "                                                     verbose=1, \n",
        "                                                     save_weights_only=False, \n",
        "                                                     save_best_only=True)\n",
        "es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                            min_delta=0, patience=10)\n",
        "history = model.fit(train_X, train_y, epochs=500, batch_size=8, \n",
        "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
        "                    callbacks=[modelckpt_callback, es_callback])\n",
        "\n",
        "loss = history.history['loss']\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af63f67b-f54a-4ad6-bffa-6d226901c8af",
        "id": "vyCQ3uer3dGy"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 7800.86377, saving model to co2.h5\n",
            "10/10 - 2s - loss: 5297.8301 - val_loss: 7800.8638 - 2s/epoch - 195ms/step\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 2: val_loss improved from 7800.86377 to 7194.67969, saving model to co2.h5\n",
            "10/10 - 0s - loss: 4823.8052 - val_loss: 7194.6797 - 91ms/epoch - 9ms/step\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 3: val_loss improved from 7194.67969 to 6436.96826, saving model to co2.h5\n",
            "10/10 - 0s - loss: 4280.7554 - val_loss: 6436.9683 - 89ms/epoch - 9ms/step\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 4: val_loss improved from 6436.96826 to 5560.52490, saving model to co2.h5\n",
            "10/10 - 0s - loss: 3614.2385 - val_loss: 5560.5249 - 82ms/epoch - 8ms/step\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 5: val_loss improved from 5560.52490 to 5052.21338, saving model to co2.h5\n",
            "10/10 - 0s - loss: 3113.3413 - val_loss: 5052.2134 - 92ms/epoch - 9ms/step\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 6: val_loss improved from 5052.21338 to 4615.85303, saving model to co2.h5\n",
            "10/10 - 0s - loss: 2774.1953 - val_loss: 4615.8530 - 91ms/epoch - 9ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: val_loss improved from 4615.85303 to 4209.80225, saving model to co2.h5\n",
            "10/10 - 0s - loss: 2459.7249 - val_loss: 4209.8022 - 93ms/epoch - 9ms/step\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 8: val_loss improved from 4209.80225 to 3818.35815, saving model to co2.h5\n",
            "10/10 - 0s - loss: 2171.8220 - val_loss: 3818.3582 - 97ms/epoch - 10ms/step\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 9: val_loss improved from 3818.35815 to 3453.70776, saving model to co2.h5\n",
            "10/10 - 0s - loss: 1896.2704 - val_loss: 3453.7078 - 98ms/epoch - 10ms/step\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 10: val_loss improved from 3453.70776 to 3130.44629, saving model to co2.h5\n",
            "10/10 - 0s - loss: 1655.7988 - val_loss: 3130.4463 - 92ms/epoch - 9ms/step\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 11: val_loss improved from 3130.44629 to 2842.15210, saving model to co2.h5\n",
            "10/10 - 0s - loss: 1442.2435 - val_loss: 2842.1521 - 81ms/epoch - 8ms/step\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 12: val_loss improved from 2842.15210 to 2574.00952, saving model to co2.h5\n",
            "10/10 - 0s - loss: 1254.3931 - val_loss: 2574.0095 - 79ms/epoch - 8ms/step\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 13: val_loss improved from 2574.00952 to 2336.91821, saving model to co2.h5\n",
            "10/10 - 0s - loss: 1087.6245 - val_loss: 2336.9182 - 88ms/epoch - 9ms/step\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 14: val_loss improved from 2336.91821 to 2133.21753, saving model to co2.h5\n",
            "10/10 - 0s - loss: 946.4238 - val_loss: 2133.2175 - 98ms/epoch - 10ms/step\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 15: val_loss improved from 2133.21753 to 1930.57825, saving model to co2.h5\n",
            "10/10 - 0s - loss: 820.2943 - val_loss: 1930.5782 - 81ms/epoch - 8ms/step\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 16: val_loss improved from 1930.57825 to 1741.93359, saving model to co2.h5\n",
            "10/10 - 0s - loss: 699.5465 - val_loss: 1741.9336 - 83ms/epoch - 8ms/step\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 17: val_loss improved from 1741.93359 to 1568.94006, saving model to co2.h5\n",
            "10/10 - 0s - loss: 594.6988 - val_loss: 1568.9401 - 82ms/epoch - 8ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: val_loss improved from 1568.94006 to 1423.83350, saving model to co2.h5\n",
            "10/10 - 0s - loss: 504.9149 - val_loss: 1423.8335 - 84ms/epoch - 8ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: val_loss improved from 1423.83350 to 1296.20459, saving model to co2.h5\n",
            "10/10 - 0s - loss: 432.3620 - val_loss: 1296.2046 - 82ms/epoch - 8ms/step\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 20: val_loss improved from 1296.20459 to 1184.83508, saving model to co2.h5\n",
            "10/10 - 0s - loss: 370.4380 - val_loss: 1184.8351 - 80ms/epoch - 8ms/step\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 21: val_loss improved from 1184.83508 to 1085.48352, saving model to co2.h5\n",
            "10/10 - 0s - loss: 318.1227 - val_loss: 1085.4835 - 83ms/epoch - 8ms/step\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 22: val_loss improved from 1085.48352 to 999.55414, saving model to co2.h5\n",
            "10/10 - 0s - loss: 275.1394 - val_loss: 999.5541 - 79ms/epoch - 8ms/step\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 23: val_loss improved from 999.55414 to 922.54858, saving model to co2.h5\n",
            "10/10 - 0s - loss: 238.9149 - val_loss: 922.5486 - 88ms/epoch - 9ms/step\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 24: val_loss improved from 922.54858 to 853.58856, saving model to co2.h5\n",
            "10/10 - 0s - loss: 208.1893 - val_loss: 853.5886 - 96ms/epoch - 10ms/step\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 25: val_loss improved from 853.58856 to 792.00977, saving model to co2.h5\n",
            "10/10 - 0s - loss: 182.3980 - val_loss: 792.0098 - 91ms/epoch - 9ms/step\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 26: val_loss improved from 792.00977 to 736.78131, saving model to co2.h5\n",
            "10/10 - 0s - loss: 160.7772 - val_loss: 736.7813 - 80ms/epoch - 8ms/step\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 27: val_loss improved from 736.78131 to 687.54352, saving model to co2.h5\n",
            "10/10 - 0s - loss: 142.7527 - val_loss: 687.5435 - 81ms/epoch - 8ms/step\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 28: val_loss improved from 687.54352 to 643.62347, saving model to co2.h5\n",
            "10/10 - 0s - loss: 127.7546 - val_loss: 643.6235 - 89ms/epoch - 9ms/step\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 29: val_loss improved from 643.62347 to 605.29688, saving model to co2.h5\n",
            "10/10 - 0s - loss: 115.6555 - val_loss: 605.2969 - 83ms/epoch - 8ms/step\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 30: val_loss improved from 605.29688 to 569.77368, saving model to co2.h5\n",
            "10/10 - 0s - loss: 105.4487 - val_loss: 569.7737 - 85ms/epoch - 9ms/step\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 31: val_loss improved from 569.77368 to 539.21832, saving model to co2.h5\n",
            "10/10 - 0s - loss: 97.3335 - val_loss: 539.2183 - 96ms/epoch - 10ms/step\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 32: val_loss improved from 539.21832 to 512.18658, saving model to co2.h5\n",
            "10/10 - 0s - loss: 90.8523 - val_loss: 512.1866 - 94ms/epoch - 9ms/step\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 33: val_loss improved from 512.18658 to 488.25452, saving model to co2.h5\n",
            "10/10 - 0s - loss: 85.6857 - val_loss: 488.2545 - 82ms/epoch - 8ms/step\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 34: val_loss improved from 488.25452 to 466.93454, saving model to co2.h5\n",
            "10/10 - 0s - loss: 81.5909 - val_loss: 466.9345 - 82ms/epoch - 8ms/step\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 35: val_loss improved from 466.93454 to 447.36377, saving model to co2.h5\n",
            "10/10 - 0s - loss: 78.3016 - val_loss: 447.3638 - 93ms/epoch - 9ms/step\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 36: val_loss improved from 447.36377 to 430.45639, saving model to co2.h5\n",
            "10/10 - 0s - loss: 75.6988 - val_loss: 430.4564 - 98ms/epoch - 10ms/step\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 37: val_loss improved from 430.45639 to 415.70200, saving model to co2.h5\n",
            "10/10 - 0s - loss: 73.7927 - val_loss: 415.7020 - 82ms/epoch - 8ms/step\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 38: val_loss improved from 415.70200 to 402.68030, saving model to co2.h5\n",
            "10/10 - 0s - loss: 72.3474 - val_loss: 402.6803 - 84ms/epoch - 8ms/step\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 39: val_loss improved from 402.68030 to 391.17264, saving model to co2.h5\n",
            "10/10 - 0s - loss: 71.2691 - val_loss: 391.1726 - 94ms/epoch - 9ms/step\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 40: val_loss improved from 391.17264 to 381.00171, saving model to co2.h5\n",
            "10/10 - 0s - loss: 70.4797 - val_loss: 381.0017 - 99ms/epoch - 10ms/step\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 41: val_loss improved from 381.00171 to 372.01401, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.9164 - val_loss: 372.0140 - 84ms/epoch - 8ms/step\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 42: val_loss improved from 372.01401 to 364.07507, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.5281 - val_loss: 364.0751 - 91ms/epoch - 9ms/step\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 43: val_loss improved from 364.07507 to 357.06644, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.2741 - val_loss: 357.0664 - 88ms/epoch - 9ms/step\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 44: val_loss improved from 357.06644 to 350.88290, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.1218 - val_loss: 350.8829 - 83ms/epoch - 8ms/step\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 45: val_loss improved from 350.88290 to 345.43225, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.0453 - val_loss: 345.4323 - 87ms/epoch - 9ms/step\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 46: val_loss improved from 345.43225 to 340.63110, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.0240 - val_loss: 340.6311 - 98ms/epoch - 10ms/step\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 47: val_loss improved from 340.63110 to 336.40622, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.0421 - val_loss: 336.4062 - 108ms/epoch - 11ms/step\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 48: val_loss improved from 336.40622 to 332.69202, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.0872 - val_loss: 332.6920 - 95ms/epoch - 10ms/step\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 49: val_loss improved from 332.69202 to 329.43054, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.1499 - val_loss: 329.4305 - 99ms/epoch - 10ms/step\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 50: val_loss improved from 329.43054 to 326.56985, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.2230 - val_loss: 326.5699 - 91ms/epoch - 9ms/step\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 51: val_loss improved from 326.56985 to 324.06351, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.3012 - val_loss: 324.0635 - 99ms/epoch - 10ms/step\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 52: val_loss improved from 324.06351 to 321.87018, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.3807 - val_loss: 321.8702 - 94ms/epoch - 9ms/step\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 53: val_loss improved from 321.87018 to 319.95322, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.4587 - val_loss: 319.9532 - 83ms/epoch - 8ms/step\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 54: val_loss improved from 319.95322 to 318.28012, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.5333 - val_loss: 318.2801 - 96ms/epoch - 10ms/step\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 55: val_loss improved from 318.28012 to 316.82196, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.6036 - val_loss: 316.8220 - 92ms/epoch - 9ms/step\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 56: val_loss improved from 316.82196 to 315.55240, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.6680 - val_loss: 315.5524 - 127ms/epoch - 13ms/step\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 57: val_loss improved from 315.55240 to 314.44885, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.7271 - val_loss: 314.4489 - 91ms/epoch - 9ms/step\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 58: val_loss improved from 314.44885 to 313.49048, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.7802 - val_loss: 313.4905 - 82ms/epoch - 8ms/step\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 59: val_loss improved from 313.49048 to 312.65945, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.8277 - val_loss: 312.6595 - 79ms/epoch - 8ms/step\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 60: val_loss improved from 312.65945 to 311.93942, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.8696 - val_loss: 311.9394 - 85ms/epoch - 9ms/step\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 61: val_loss improved from 311.93942 to 311.31686, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.9064 - val_loss: 311.3169 - 79ms/epoch - 8ms/step\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 62: val_loss improved from 311.31686 to 310.77982, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.9385 - val_loss: 310.7798 - 89ms/epoch - 9ms/step\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 63: val_loss improved from 310.77982 to 310.31656, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.9652 - val_loss: 310.3166 - 83ms/epoch - 8ms/step\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 64: val_loss improved from 310.31656 to 309.92148, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.9884 - val_loss: 309.9215 - 90ms/epoch - 9ms/step\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 65: val_loss improved from 309.92148 to 309.57559, saving model to co2.h5\n",
            "10/10 - 0s - loss: 70.5306 - val_loss: 309.5756 - 92ms/epoch - 9ms/step\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 66: val_loss improved from 309.57559 to 309.02850, saving model to co2.h5\n",
            "10/10 - 0s - loss: 69.5423 - val_loss: 309.0285 - 84ms/epoch - 8ms/step\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 67: val_loss did not improve from 309.02850\n",
            "10/10 - 0s - loss: 70.0766 - val_loss: 315.1661 - 67ms/epoch - 7ms/step\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 68: val_loss did not improve from 309.02850\n",
            "10/10 - 0s - loss: 71.1238 - val_loss: 333.9637 - 69ms/epoch - 7ms/step\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 69: val_loss did not improve from 309.02850\n",
            "10/10 - 0s - loss: 67.5039 - val_loss: 319.0125 - 61ms/epoch - 6ms/step\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 70: val_loss did not improve from 309.02850\n",
            "10/10 - 0s - loss: 67.9031 - val_loss: 313.5621 - 73ms/epoch - 7ms/step\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 71: val_loss improved from 309.02850 to 305.50702, saving model to co2.h5\n",
            "10/10 - 0s - loss: 67.9500 - val_loss: 305.5070 - 83ms/epoch - 8ms/step\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 72: val_loss improved from 305.50702 to 302.50916, saving model to co2.h5\n",
            "10/10 - 0s - loss: 66.4731 - val_loss: 302.5092 - 84ms/epoch - 8ms/step\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 73: val_loss did not improve from 302.50916\n",
            "10/10 - 0s - loss: 69.2782 - val_loss: 323.7406 - 69ms/epoch - 7ms/step\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 74: val_loss did not improve from 302.50916\n",
            "10/10 - 0s - loss: 63.5133 - val_loss: 309.3878 - 70ms/epoch - 7ms/step\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 75: val_loss did not improve from 302.50916\n",
            "10/10 - 0s - loss: 64.1978 - val_loss: 305.5555 - 58ms/epoch - 6ms/step\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 76: val_loss improved from 302.50916 to 301.56592, saving model to co2.h5\n",
            "10/10 - 0s - loss: 63.3364 - val_loss: 301.5659 - 98ms/epoch - 10ms/step\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 77: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 62.8363 - val_loss: 307.4690 - 74ms/epoch - 7ms/step\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 78: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 64.0659 - val_loss: 320.1944 - 59ms/epoch - 6ms/step\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 79: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 61.3914 - val_loss: 313.6886 - 57ms/epoch - 6ms/step\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 80: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 59.8071 - val_loss: 305.2750 - 55ms/epoch - 6ms/step\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 81: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 60.6830 - val_loss: 307.3751 - 58ms/epoch - 6ms/step\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 82: val_loss did not improve from 301.56592\n",
            "10/10 - 0s - loss: 59.1055 - val_loss: 306.4773 - 64ms/epoch - 6ms/step\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 83: val_loss improved from 301.56592 to 298.60312, saving model to co2.h5\n",
            "10/10 - 0s - loss: 57.7480 - val_loss: 298.6031 - 85ms/epoch - 9ms/step\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 84: val_loss improved from 298.60312 to 285.76096, saving model to co2.h5\n",
            "10/10 - 0s - loss: 56.8515 - val_loss: 285.7610 - 97ms/epoch - 10ms/step\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 85: val_loss improved from 285.76096 to 277.26688, saving model to co2.h5\n",
            "10/10 - 0s - loss: 54.6909 - val_loss: 277.2669 - 92ms/epoch - 9ms/step\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 86: val_loss improved from 277.26688 to 270.58380, saving model to co2.h5\n",
            "10/10 - 0s - loss: 54.4763 - val_loss: 270.5838 - 91ms/epoch - 9ms/step\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 87: val_loss improved from 270.58380 to 264.23352, saving model to co2.h5\n",
            "10/10 - 0s - loss: 52.8480 - val_loss: 264.2335 - 83ms/epoch - 8ms/step\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 88: val_loss improved from 264.23352 to 257.52756, saving model to co2.h5\n",
            "10/10 - 0s - loss: 51.8937 - val_loss: 257.5276 - 81ms/epoch - 8ms/step\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 89: val_loss improved from 257.52756 to 251.23512, saving model to co2.h5\n",
            "10/10 - 0s - loss: 50.1566 - val_loss: 251.2351 - 81ms/epoch - 8ms/step\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 90: val_loss improved from 251.23512 to 243.66571, saving model to co2.h5\n",
            "10/10 - 0s - loss: 49.2062 - val_loss: 243.6657 - 99ms/epoch - 10ms/step\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 91: val_loss improved from 243.66571 to 238.90517, saving model to co2.h5\n",
            "10/10 - 0s - loss: 48.4266 - val_loss: 238.9052 - 97ms/epoch - 10ms/step\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 92: val_loss improved from 238.90517 to 231.89006, saving model to co2.h5\n",
            "10/10 - 0s - loss: 46.8086 - val_loss: 231.8901 - 85ms/epoch - 9ms/step\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 93: val_loss improved from 231.89006 to 226.92616, saving model to co2.h5\n",
            "10/10 - 0s - loss: 46.1224 - val_loss: 226.9262 - 101ms/epoch - 10ms/step\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 94: val_loss improved from 226.92616 to 220.25188, saving model to co2.h5\n",
            "10/10 - 0s - loss: 44.7333 - val_loss: 220.2519 - 83ms/epoch - 8ms/step\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 95: val_loss improved from 220.25188 to 215.54366, saving model to co2.h5\n",
            "10/10 - 0s - loss: 44.1478 - val_loss: 215.5437 - 85ms/epoch - 9ms/step\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 96: val_loss improved from 215.54366 to 209.27344, saving model to co2.h5\n",
            "10/10 - 0s - loss: 42.7611 - val_loss: 209.2734 - 95ms/epoch - 9ms/step\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 97: val_loss improved from 209.27344 to 204.69270, saving model to co2.h5\n",
            "10/10 - 0s - loss: 42.2956 - val_loss: 204.6927 - 93ms/epoch - 9ms/step\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 98: val_loss improved from 204.69270 to 198.55501, saving model to co2.h5\n",
            "10/10 - 0s - loss: 40.9406 - val_loss: 198.5550 - 95ms/epoch - 9ms/step\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 99: val_loss improved from 198.55501 to 194.84837, saving model to co2.h5\n",
            "10/10 - 0s - loss: 40.9805 - val_loss: 194.8484 - 94ms/epoch - 9ms/step\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 100: val_loss improved from 194.84837 to 189.34352, saving model to co2.h5\n",
            "10/10 - 0s - loss: 39.3265 - val_loss: 189.3435 - 87ms/epoch - 9ms/step\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 101: val_loss improved from 189.34352 to 185.02730, saving model to co2.h5\n",
            "10/10 - 0s - loss: 39.1887 - val_loss: 185.0273 - 96ms/epoch - 10ms/step\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 102: val_loss improved from 185.02730 to 180.51204, saving model to co2.h5\n",
            "10/10 - 0s - loss: 38.3923 - val_loss: 180.5120 - 88ms/epoch - 9ms/step\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 103: val_loss improved from 180.51204 to 175.49536, saving model to co2.h5\n",
            "10/10 - 0s - loss: 37.6837 - val_loss: 175.4954 - 95ms/epoch - 9ms/step\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 104: val_loss improved from 175.49536 to 165.27034, saving model to co2.h5\n",
            "10/10 - 0s - loss: 36.6462 - val_loss: 165.2703 - 99ms/epoch - 10ms/step\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 105: val_loss improved from 165.27034 to 163.08635, saving model to co2.h5\n",
            "10/10 - 0s - loss: 36.9433 - val_loss: 163.0863 - 100ms/epoch - 10ms/step\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 106: val_loss improved from 163.08635 to 158.26881, saving model to co2.h5\n",
            "10/10 - 0s - loss: 35.1329 - val_loss: 158.2688 - 90ms/epoch - 9ms/step\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 107: val_loss improved from 158.26881 to 155.15103, saving model to co2.h5\n",
            "10/10 - 0s - loss: 35.6327 - val_loss: 155.1510 - 96ms/epoch - 10ms/step\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 108: val_loss improved from 155.15103 to 152.01326, saving model to co2.h5\n",
            "10/10 - 0s - loss: 34.5517 - val_loss: 152.0133 - 91ms/epoch - 9ms/step\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 109: val_loss improved from 152.01326 to 148.72195, saving model to co2.h5\n",
            "10/10 - 0s - loss: 34.2385 - val_loss: 148.7220 - 91ms/epoch - 9ms/step\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 110: val_loss improved from 148.72195 to 145.77797, saving model to co2.h5\n",
            "10/10 - 0s - loss: 33.9200 - val_loss: 145.7780 - 102ms/epoch - 10ms/step\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 111: val_loss improved from 145.77797 to 142.75883, saving model to co2.h5\n",
            "10/10 - 1s - loss: 33.4415 - val_loss: 142.7588 - 521ms/epoch - 52ms/step\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 112: val_loss improved from 142.75883 to 140.11812, saving model to co2.h5\n",
            "10/10 - 0s - loss: 33.2287 - val_loss: 140.1181 - 97ms/epoch - 10ms/step\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 113: val_loss improved from 140.11812 to 137.29010, saving model to co2.h5\n",
            "10/10 - 0s - loss: 32.7911 - val_loss: 137.2901 - 86ms/epoch - 9ms/step\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 114: val_loss improved from 137.29010 to 134.80296, saving model to co2.h5\n",
            "10/10 - 0s - loss: 32.6261 - val_loss: 134.8030 - 91ms/epoch - 9ms/step\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 115: val_loss improved from 134.80296 to 132.25621, saving model to co2.h5\n",
            "10/10 - 0s - loss: 32.2628 - val_loss: 132.2562 - 93ms/epoch - 9ms/step\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 116: val_loss improved from 132.25621 to 129.87401, saving model to co2.h5\n",
            "10/10 - 0s - loss: 32.0597 - val_loss: 129.8740 - 82ms/epoch - 8ms/step\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 117: val_loss improved from 129.87401 to 127.50761, saving model to co2.h5\n",
            "10/10 - 0s - loss: 31.7999 - val_loss: 127.5076 - 82ms/epoch - 8ms/step\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 118: val_loss improved from 127.50761 to 125.17905, saving model to co2.h5\n",
            "10/10 - 0s - loss: 31.6025 - val_loss: 125.1791 - 85ms/epoch - 9ms/step\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 119: val_loss improved from 125.17905 to 122.05116, saving model to co2.h5\n",
            "10/10 - 0s - loss: 31.3488 - val_loss: 122.0512 - 83ms/epoch - 8ms/step\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 120: val_loss improved from 122.05116 to 116.88318, saving model to co2.h5\n",
            "10/10 - 0s - loss: 31.0532 - val_loss: 116.8832 - 87ms/epoch - 9ms/step\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 121: val_loss improved from 116.88318 to 115.70794, saving model to co2.h5\n",
            "10/10 - 0s - loss: 31.1099 - val_loss: 115.7079 - 81ms/epoch - 8ms/step\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 122: val_loss improved from 115.70794 to 113.84705, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.6021 - val_loss: 113.8470 - 85ms/epoch - 9ms/step\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 123: val_loss improved from 113.84705 to 112.29983, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.6032 - val_loss: 112.2998 - 93ms/epoch - 9ms/step\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 124: val_loss improved from 112.29983 to 110.53666, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.3542 - val_loss: 110.5367 - 87ms/epoch - 9ms/step\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 125: val_loss improved from 110.53666 to 109.12885, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.3845 - val_loss: 109.1288 - 81ms/epoch - 8ms/step\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 126: val_loss improved from 109.12885 to 107.65382, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.1676 - val_loss: 107.6538 - 80ms/epoch - 8ms/step\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 127: val_loss improved from 107.65382 to 106.32188, saving model to co2.h5\n",
            "10/10 - 0s - loss: 30.0996 - val_loss: 106.3219 - 95ms/epoch - 9ms/step\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 128: val_loss improved from 106.32188 to 105.01489, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.9605 - val_loss: 105.0149 - 96ms/epoch - 10ms/step\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 129: val_loss improved from 105.01489 to 103.78341, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.8626 - val_loss: 103.7834 - 79ms/epoch - 8ms/step\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 130: val_loss improved from 103.78341 to 102.60154, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.7581 - val_loss: 102.6015 - 78ms/epoch - 8ms/step\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 131: val_loss improved from 102.60154 to 101.45475, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.6623 - val_loss: 101.4548 - 90ms/epoch - 9ms/step\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 132: val_loss improved from 101.45475 to 100.36414, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.5842 - val_loss: 100.3641 - 96ms/epoch - 10ms/step\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 133: val_loss improved from 100.36414 to 99.31561, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.5039 - val_loss: 99.3156 - 89ms/epoch - 9ms/step\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 134: val_loss improved from 99.31561 to 98.30640, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.4292 - val_loss: 98.3064 - 87ms/epoch - 9ms/step\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 135: val_loss improved from 98.30640 to 97.33868, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.3615 - val_loss: 97.3387 - 92ms/epoch - 9ms/step\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 136: val_loss improved from 97.33868 to 96.40888, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.2972 - val_loss: 96.4089 - 94ms/epoch - 9ms/step\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 137: val_loss improved from 96.40888 to 95.51495, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.2366 - val_loss: 95.5150 - 84ms/epoch - 8ms/step\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 138: val_loss improved from 95.51495 to 94.65540, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.1800 - val_loss: 94.6554 - 89ms/epoch - 9ms/step\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 139: val_loss improved from 94.65540 to 93.82904, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.1266 - val_loss: 93.8290 - 84ms/epoch - 8ms/step\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 140: val_loss improved from 93.82904 to 93.03333, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.0761 - val_loss: 93.0333 - 88ms/epoch - 9ms/step\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 141: val_loss improved from 93.03333 to 92.26781, saving model to co2.h5\n",
            "10/10 - 0s - loss: 29.0286 - val_loss: 92.2678 - 85ms/epoch - 8ms/step\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 142: val_loss improved from 92.26781 to 91.52969, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.9836 - val_loss: 91.5297 - 96ms/epoch - 10ms/step\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 143: val_loss improved from 91.52969 to 90.81954, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.9412 - val_loss: 90.8195 - 83ms/epoch - 8ms/step\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 144: val_loss improved from 90.81954 to 90.13360, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.9009 - val_loss: 90.1336 - 81ms/epoch - 8ms/step\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 145: val_loss improved from 90.13360 to 89.47382, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.8628 - val_loss: 89.4738 - 93ms/epoch - 9ms/step\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 146: val_loss improved from 89.47382 to 88.83515, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.8265 - val_loss: 88.8352 - 87ms/epoch - 9ms/step\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 147: val_loss improved from 88.83515 to 88.22128, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.7922 - val_loss: 88.2213 - 90ms/epoch - 9ms/step\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 148: val_loss improved from 88.22128 to 87.62589, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.7593 - val_loss: 87.6259 - 93ms/epoch - 9ms/step\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 149: val_loss improved from 87.62589 to 87.05350, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.7283 - val_loss: 87.0535 - 86ms/epoch - 9ms/step\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 150: val_loss improved from 87.05350 to 86.49781, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.6985 - val_loss: 86.4978 - 95ms/epoch - 10ms/step\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 151: val_loss improved from 86.49781 to 85.96326, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.6703 - val_loss: 85.9633 - 85ms/epoch - 8ms/step\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 152: val_loss improved from 85.96326 to 85.44407, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.6431 - val_loss: 85.4441 - 95ms/epoch - 10ms/step\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 153: val_loss improved from 85.44407 to 84.94377, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.6174 - val_loss: 84.9438 - 97ms/epoch - 10ms/step\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 154: val_loss improved from 84.94377 to 84.45820, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.5925 - val_loss: 84.4582 - 82ms/epoch - 8ms/step\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 155: val_loss improved from 84.45820 to 83.98930, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.5689 - val_loss: 83.9893 - 85ms/epoch - 8ms/step\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 156: val_loss improved from 83.98930 to 83.53447, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.5461 - val_loss: 83.5345 - 121ms/epoch - 12ms/step\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 157: val_loss improved from 83.53447 to 83.09455, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.5245 - val_loss: 83.0946 - 136ms/epoch - 14ms/step\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 158: val_loss improved from 83.09455 to 82.66785, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.5036 - val_loss: 82.6679 - 123ms/epoch - 12ms/step\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 159: val_loss improved from 82.66785 to 82.25474, saving model to co2.h5\n",
            "10/10 - 1s - loss: 28.4836 - val_loss: 82.2547 - 544ms/epoch - 54ms/step\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 160: val_loss improved from 82.25474 to 81.85395, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.4644 - val_loss: 81.8540 - 143ms/epoch - 14ms/step\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 161: val_loss improved from 81.85395 to 81.46557, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.4459 - val_loss: 81.4656 - 138ms/epoch - 14ms/step\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 162: val_loss improved from 81.46557 to 81.08862, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.4281 - val_loss: 81.0886 - 128ms/epoch - 13ms/step\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 163: val_loss improved from 81.08862 to 80.72306, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.4109 - val_loss: 80.7231 - 117ms/epoch - 12ms/step\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 164: val_loss improved from 80.72306 to 80.36793, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3943 - val_loss: 80.3679 - 128ms/epoch - 13ms/step\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 165: val_loss improved from 80.36793 to 80.02351, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3784 - val_loss: 80.0235 - 155ms/epoch - 16ms/step\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 166: val_loss improved from 80.02351 to 79.68844, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3629 - val_loss: 79.6884 - 126ms/epoch - 13ms/step\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 167: val_loss improved from 79.68844 to 79.36356, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3481 - val_loss: 79.3636 - 124ms/epoch - 12ms/step\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 168: val_loss improved from 79.36356 to 79.04715, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3337 - val_loss: 79.0471 - 124ms/epoch - 12ms/step\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 169: val_loss improved from 79.04715 to 78.74029, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3197 - val_loss: 78.7403 - 160ms/epoch - 16ms/step\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 170: val_loss improved from 78.74029 to 78.44084, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.3062 - val_loss: 78.4408 - 124ms/epoch - 12ms/step\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 171: val_loss improved from 78.44084 to 78.15114, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2932 - val_loss: 78.1511 - 127ms/epoch - 13ms/step\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 172: val_loss improved from 78.15114 to 77.86700, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2806 - val_loss: 77.8670 - 129ms/epoch - 13ms/step\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 173: val_loss improved from 77.86700 to 77.59348, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2684 - val_loss: 77.5935 - 133ms/epoch - 13ms/step\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 174: val_loss improved from 77.59348 to 77.32362, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2567 - val_loss: 77.3236 - 124ms/epoch - 12ms/step\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 175: val_loss improved from 77.32362 to 77.06513, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2451 - val_loss: 77.0651 - 136ms/epoch - 14ms/step\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 176: val_loss improved from 77.06513 to 76.80909, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2338 - val_loss: 76.8091 - 145ms/epoch - 14ms/step\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 177: val_loss improved from 76.80909 to 76.56333, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2228 - val_loss: 76.5633 - 152ms/epoch - 15ms/step\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 178: val_loss improved from 76.56333 to 76.32026, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2122 - val_loss: 76.3203 - 249ms/epoch - 25ms/step\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 179: val_loss improved from 76.32026 to 76.08681, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.2021 - val_loss: 76.0868 - 279ms/epoch - 28ms/step\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 180: val_loss improved from 76.08681 to 75.85621, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1919 - val_loss: 75.8562 - 153ms/epoch - 15ms/step\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 181: val_loss improved from 75.85621 to 75.63390, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1821 - val_loss: 75.6339 - 90ms/epoch - 9ms/step\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 182: val_loss improved from 75.63390 to 75.41489, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1723 - val_loss: 75.4149 - 95ms/epoch - 9ms/step\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 183: val_loss improved from 75.41489 to 75.20245, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1631 - val_loss: 75.2025 - 93ms/epoch - 9ms/step\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 184: val_loss improved from 75.20245 to 74.99416, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1539 - val_loss: 74.9942 - 101ms/epoch - 10ms/step\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 185: val_loss improved from 74.99416 to 74.79122, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1452 - val_loss: 74.7912 - 86ms/epoch - 9ms/step\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 186: val_loss improved from 74.79122 to 74.59276, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1366 - val_loss: 74.5928 - 94ms/epoch - 9ms/step\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 187: val_loss improved from 74.59276 to 74.39878, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1283 - val_loss: 74.3988 - 94ms/epoch - 9ms/step\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 188: val_loss improved from 74.39878 to 74.20934, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1202 - val_loss: 74.2093 - 80ms/epoch - 8ms/step\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 189: val_loss improved from 74.20934 to 74.02395, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1123 - val_loss: 74.0239 - 84ms/epoch - 8ms/step\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 190: val_loss improved from 74.02395 to 73.84267, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.1046 - val_loss: 73.8427 - 88ms/epoch - 9ms/step\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 191: val_loss improved from 73.84267 to 73.66531, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0971 - val_loss: 73.6653 - 87ms/epoch - 9ms/step\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 192: val_loss improved from 73.66531 to 73.49174, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0898 - val_loss: 73.4917 - 93ms/epoch - 9ms/step\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 193: val_loss improved from 73.49174 to 73.32171, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0827 - val_loss: 73.3217 - 88ms/epoch - 9ms/step\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 194: val_loss improved from 73.32171 to 73.15540, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0758 - val_loss: 73.1554 - 99ms/epoch - 10ms/step\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 195: val_loss improved from 73.15540 to 72.99223, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0690 - val_loss: 72.9922 - 362ms/epoch - 36ms/step\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 196: val_loss improved from 72.99223 to 72.83263, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0624 - val_loss: 72.8326 - 108ms/epoch - 11ms/step\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 197: val_loss improved from 72.83263 to 72.67582, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0560 - val_loss: 72.6758 - 92ms/epoch - 9ms/step\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 198: val_loss improved from 72.67582 to 72.52255, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0498 - val_loss: 72.5226 - 84ms/epoch - 8ms/step\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 199: val_loss improved from 72.52255 to 72.37148, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0437 - val_loss: 72.3715 - 80ms/epoch - 8ms/step\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 200: val_loss improved from 72.37148 to 72.22448, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0378 - val_loss: 72.2245 - 81ms/epoch - 8ms/step\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 201: val_loss improved from 72.22448 to 72.07832, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0321 - val_loss: 72.0783 - 90ms/epoch - 9ms/step\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 202: val_loss improved from 72.07832 to 71.93732, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0266 - val_loss: 71.9373 - 86ms/epoch - 9ms/step\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 203: val_loss improved from 71.93732 to 71.79545, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0214 - val_loss: 71.7954 - 97ms/epoch - 10ms/step\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 204: val_loss improved from 71.79545 to 71.66061, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0162 - val_loss: 71.6606 - 89ms/epoch - 9ms/step\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 205: val_loss improved from 71.66061 to 71.52460, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0110 - val_loss: 71.5246 - 87ms/epoch - 9ms/step\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 206: val_loss improved from 71.52460 to 71.39367, saving model to co2.h5\n",
            "10/10 - 0s - loss: 28.0052 - val_loss: 71.3937 - 107ms/epoch - 11ms/step\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 207: val_loss improved from 71.39367 to 71.26308, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9996 - val_loss: 71.2631 - 95ms/epoch - 10ms/step\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 208: val_loss improved from 71.26308 to 71.13457, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9946 - val_loss: 71.1346 - 98ms/epoch - 10ms/step\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 209: val_loss improved from 71.13457 to 71.00885, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9896 - val_loss: 71.0089 - 90ms/epoch - 9ms/step\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 210: val_loss improved from 71.00885 to 70.88238, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9853 - val_loss: 70.8824 - 81ms/epoch - 8ms/step\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 211: val_loss improved from 70.88238 to 70.76162, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9812 - val_loss: 70.7616 - 96ms/epoch - 10ms/step\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 212: val_loss improved from 70.76162 to 70.64106, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9769 - val_loss: 70.6411 - 86ms/epoch - 9ms/step\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 213: val_loss improved from 70.64106 to 70.52523, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9713 - val_loss: 70.5252 - 82ms/epoch - 8ms/step\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 214: val_loss improved from 70.52523 to 70.40842, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9653 - val_loss: 70.4084 - 87ms/epoch - 9ms/step\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 215: val_loss improved from 70.40842 to 70.29449, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9609 - val_loss: 70.2945 - 92ms/epoch - 9ms/step\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 216: val_loss improved from 70.29449 to 70.18295, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9564 - val_loss: 70.1830 - 91ms/epoch - 9ms/step\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 217: val_loss improved from 70.18295 to 70.07346, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9522 - val_loss: 70.0735 - 82ms/epoch - 8ms/step\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 218: val_loss improved from 70.07346 to 69.96560, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9477 - val_loss: 69.9656 - 82ms/epoch - 8ms/step\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 219: val_loss improved from 69.96560 to 69.85899, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9433 - val_loss: 69.8590 - 84ms/epoch - 8ms/step\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 220: val_loss improved from 69.85899 to 69.75388, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9389 - val_loss: 69.7539 - 81ms/epoch - 8ms/step\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 221: val_loss improved from 69.75388 to 69.65026, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9346 - val_loss: 69.6503 - 90ms/epoch - 9ms/step\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 222: val_loss improved from 69.65026 to 69.54818, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9302 - val_loss: 69.5482 - 85ms/epoch - 9ms/step\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 223: val_loss improved from 69.54818 to 69.44764, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9258 - val_loss: 69.4476 - 81ms/epoch - 8ms/step\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 224: val_loss improved from 69.44764 to 69.34862, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9215 - val_loss: 69.3486 - 93ms/epoch - 9ms/step\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 225: val_loss improved from 69.34862 to 69.25107, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9171 - val_loss: 69.2511 - 92ms/epoch - 9ms/step\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 226: val_loss improved from 69.25107 to 69.15513, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9128 - val_loss: 69.1551 - 94ms/epoch - 9ms/step\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 227: val_loss improved from 69.15513 to 69.06052, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9084 - val_loss: 69.0605 - 94ms/epoch - 9ms/step\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 228: val_loss improved from 69.06052 to 68.96743, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.9040 - val_loss: 68.9674 - 93ms/epoch - 9ms/step\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 229: val_loss improved from 68.96743 to 68.87571, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8996 - val_loss: 68.8757 - 97ms/epoch - 10ms/step\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 230: val_loss improved from 68.87571 to 68.78549, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8952 - val_loss: 68.7855 - 102ms/epoch - 10ms/step\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 231: val_loss improved from 68.78549 to 68.69645, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8908 - val_loss: 68.6964 - 95ms/epoch - 9ms/step\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 232: val_loss improved from 68.69645 to 68.60903, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8864 - val_loss: 68.6090 - 85ms/epoch - 9ms/step\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 233: val_loss improved from 68.60903 to 68.52225, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8819 - val_loss: 68.5222 - 85ms/epoch - 8ms/step\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 234: val_loss improved from 68.52225 to 68.43730, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8773 - val_loss: 68.4373 - 83ms/epoch - 8ms/step\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 235: val_loss improved from 68.43730 to 68.35204, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8727 - val_loss: 68.3520 - 90ms/epoch - 9ms/step\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 236: val_loss improved from 68.35204 to 68.26902, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8679 - val_loss: 68.2690 - 93ms/epoch - 9ms/step\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 237: val_loss improved from 68.26902 to 68.18356, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8635 - val_loss: 68.1836 - 90ms/epoch - 9ms/step\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 238: val_loss improved from 68.18356 to 68.10342, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8596 - val_loss: 68.1034 - 106ms/epoch - 11ms/step\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 239: val_loss improved from 68.10342 to 68.02358, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8568 - val_loss: 68.0236 - 89ms/epoch - 9ms/step\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 240: val_loss improved from 68.02358 to 67.95173, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8531 - val_loss: 67.9517 - 93ms/epoch - 9ms/step\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 241: val_loss improved from 67.95173 to 67.87765, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8481 - val_loss: 67.8776 - 101ms/epoch - 10ms/step\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 242: val_loss improved from 67.87765 to 67.80392, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8439 - val_loss: 67.8039 - 81ms/epoch - 8ms/step\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 243: val_loss improved from 67.80392 to 67.73181, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8398 - val_loss: 67.7318 - 85ms/epoch - 9ms/step\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 244: val_loss improved from 67.73181 to 67.65775, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8365 - val_loss: 67.6578 - 97ms/epoch - 10ms/step\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 245: val_loss improved from 67.65775 to 67.58825, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8328 - val_loss: 67.5882 - 138ms/epoch - 14ms/step\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 246: val_loss improved from 67.58825 to 67.51460, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8298 - val_loss: 67.5146 - 99ms/epoch - 10ms/step\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 247: val_loss improved from 67.51460 to 67.44749, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8264 - val_loss: 67.4475 - 95ms/epoch - 10ms/step\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 248: val_loss improved from 67.44749 to 67.37672, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8236 - val_loss: 67.3767 - 81ms/epoch - 8ms/step\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 249: val_loss improved from 67.37672 to 67.31187, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8200 - val_loss: 67.3119 - 80ms/epoch - 8ms/step\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 250: val_loss improved from 67.31187 to 67.24616, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8157 - val_loss: 67.2462 - 94ms/epoch - 9ms/step\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 251: val_loss improved from 67.24616 to 67.18200, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8109 - val_loss: 67.1820 - 80ms/epoch - 8ms/step\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 252: val_loss improved from 67.18200 to 67.11758, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8062 - val_loss: 67.1176 - 84ms/epoch - 8ms/step\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 253: val_loss improved from 67.11758 to 67.05444, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.8026 - val_loss: 67.0544 - 90ms/epoch - 9ms/step\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 254: val_loss improved from 67.05444 to 66.99192, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7987 - val_loss: 66.9919 - 88ms/epoch - 9ms/step\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 255: val_loss improved from 66.99192 to 66.93066, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7953 - val_loss: 66.9307 - 85ms/epoch - 9ms/step\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 256: val_loss improved from 66.93066 to 66.86999, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7916 - val_loss: 66.8700 - 84ms/epoch - 8ms/step\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 257: val_loss improved from 66.86999 to 66.81039, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7881 - val_loss: 66.8104 - 96ms/epoch - 10ms/step\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 258: val_loss improved from 66.81039 to 66.75167, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7845 - val_loss: 66.7517 - 92ms/epoch - 9ms/step\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 259: val_loss improved from 66.75167 to 66.69379, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7811 - val_loss: 66.6938 - 98ms/epoch - 10ms/step\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 260: val_loss improved from 66.69379 to 66.63670, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7775 - val_loss: 66.6367 - 92ms/epoch - 9ms/step\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 261: val_loss improved from 66.63670 to 66.58051, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7741 - val_loss: 66.5805 - 82ms/epoch - 8ms/step\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 262: val_loss improved from 66.58051 to 66.52520, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7706 - val_loss: 66.5252 - 81ms/epoch - 8ms/step\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 263: val_loss improved from 66.52520 to 66.47068, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7672 - val_loss: 66.4707 - 81ms/epoch - 8ms/step\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 264: val_loss improved from 66.47068 to 66.41702, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7638 - val_loss: 66.4170 - 94ms/epoch - 9ms/step\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 265: val_loss improved from 66.41702 to 66.36412, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7604 - val_loss: 66.3641 - 95ms/epoch - 10ms/step\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 266: val_loss improved from 66.36412 to 66.31203, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7570 - val_loss: 66.3120 - 90ms/epoch - 9ms/step\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 267: val_loss improved from 66.31203 to 66.26066, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7537 - val_loss: 66.2607 - 82ms/epoch - 8ms/step\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 268: val_loss improved from 66.26066 to 66.21011, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7504 - val_loss: 66.2101 - 92ms/epoch - 9ms/step\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 269: val_loss improved from 66.21011 to 66.16029, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7471 - val_loss: 66.1603 - 95ms/epoch - 9ms/step\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 270: val_loss improved from 66.16029 to 66.11122, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7438 - val_loss: 66.1112 - 88ms/epoch - 9ms/step\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 271: val_loss improved from 66.11122 to 66.06266, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7405 - val_loss: 66.0627 - 97ms/epoch - 10ms/step\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 272: val_loss improved from 66.06266 to 66.01512, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7373 - val_loss: 66.0151 - 93ms/epoch - 9ms/step\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 273: val_loss improved from 66.01512 to 65.96783, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7341 - val_loss: 65.9678 - 82ms/epoch - 8ms/step\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 274: val_loss improved from 65.96783 to 65.92190, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7308 - val_loss: 65.9219 - 106ms/epoch - 11ms/step\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 275: val_loss improved from 65.92190 to 65.87554, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7277 - val_loss: 65.8755 - 90ms/epoch - 9ms/step\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 276: val_loss improved from 65.87554 to 65.83109, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7246 - val_loss: 65.8311 - 95ms/epoch - 9ms/step\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 277: val_loss improved from 65.83109 to 65.78504, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7217 - val_loss: 65.7850 - 98ms/epoch - 10ms/step\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 278: val_loss improved from 65.78504 to 65.74232, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7189 - val_loss: 65.7423 - 101ms/epoch - 10ms/step\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 279: val_loss improved from 65.74232 to 65.69709, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7166 - val_loss: 65.6971 - 86ms/epoch - 9ms/step\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 280: val_loss improved from 65.69709 to 65.65775, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7136 - val_loss: 65.6577 - 97ms/epoch - 10ms/step\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 281: val_loss improved from 65.65775 to 65.61395, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7098 - val_loss: 65.6139 - 84ms/epoch - 8ms/step\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 282: val_loss improved from 65.61395 to 65.57430, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7063 - val_loss: 65.5743 - 90ms/epoch - 9ms/step\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 283: val_loss improved from 65.57430 to 65.52926, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7036 - val_loss: 65.5293 - 106ms/epoch - 11ms/step\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 284: val_loss improved from 65.52926 to 65.49024, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7016 - val_loss: 65.4902 - 90ms/epoch - 9ms/step\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 285: val_loss improved from 65.49024 to 65.44305, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7012 - val_loss: 65.4431 - 100ms/epoch - 10ms/step\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 286: val_loss improved from 65.44305 to 65.41111, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.7001 - val_loss: 65.4111 - 88ms/epoch - 9ms/step\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 287: val_loss improved from 65.41111 to 65.37150, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6972 - val_loss: 65.3715 - 83ms/epoch - 8ms/step\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 288: val_loss improved from 65.37150 to 65.33860, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6901 - val_loss: 65.3386 - 89ms/epoch - 9ms/step\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 289: val_loss improved from 65.33860 to 65.30044, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6856 - val_loss: 65.3004 - 91ms/epoch - 9ms/step\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 290: val_loss improved from 65.30044 to 65.26354, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6818 - val_loss: 65.2635 - 87ms/epoch - 9ms/step\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 291: val_loss improved from 65.26354 to 65.22815, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6792 - val_loss: 65.2281 - 85ms/epoch - 9ms/step\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 292: val_loss improved from 65.22815 to 65.19195, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6763 - val_loss: 65.1919 - 91ms/epoch - 9ms/step\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 293: val_loss improved from 65.19195 to 65.15703, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6735 - val_loss: 65.1570 - 90ms/epoch - 9ms/step\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 294: val_loss improved from 65.15703 to 65.12217, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6706 - val_loss: 65.1222 - 90ms/epoch - 9ms/step\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 295: val_loss improved from 65.12217 to 65.08795, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6678 - val_loss: 65.0880 - 91ms/epoch - 9ms/step\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 296: val_loss improved from 65.08795 to 65.05415, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6651 - val_loss: 65.0542 - 96ms/epoch - 10ms/step\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 297: val_loss improved from 65.05415 to 65.02077, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6623 - val_loss: 65.0208 - 91ms/epoch - 9ms/step\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 298: val_loss improved from 65.02077 to 64.98787, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6596 - val_loss: 64.9879 - 91ms/epoch - 9ms/step\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 299: val_loss improved from 64.98787 to 64.95538, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6568 - val_loss: 64.9554 - 95ms/epoch - 10ms/step\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 300: val_loss improved from 64.95538 to 64.92332, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6541 - val_loss: 64.9233 - 101ms/epoch - 10ms/step\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 301: val_loss improved from 64.92332 to 64.89167, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6514 - val_loss: 64.8917 - 91ms/epoch - 9ms/step\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 302: val_loss improved from 64.89167 to 64.86050, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6487 - val_loss: 64.8605 - 85ms/epoch - 9ms/step\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 303: val_loss improved from 64.86050 to 64.82970, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6461 - val_loss: 64.8297 - 89ms/epoch - 9ms/step\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 304: val_loss improved from 64.82970 to 64.79935, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6434 - val_loss: 64.7993 - 82ms/epoch - 8ms/step\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 305: val_loss improved from 64.79935 to 64.76936, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6408 - val_loss: 64.7694 - 99ms/epoch - 10ms/step\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 306: val_loss improved from 64.76936 to 64.73981, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6382 - val_loss: 64.7398 - 95ms/epoch - 9ms/step\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 307: val_loss improved from 64.73981 to 64.71064, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6356 - val_loss: 64.7106 - 97ms/epoch - 10ms/step\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 308: val_loss improved from 64.71064 to 64.68185, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6330 - val_loss: 64.6818 - 96ms/epoch - 10ms/step\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 309: val_loss improved from 64.68185 to 64.65346, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6304 - val_loss: 64.6535 - 83ms/epoch - 8ms/step\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 310: val_loss improved from 64.65346 to 64.62540, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6279 - val_loss: 64.6254 - 111ms/epoch - 11ms/step\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 311: val_loss improved from 64.62540 to 64.59789, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6253 - val_loss: 64.5979 - 93ms/epoch - 9ms/step\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 312: val_loss improved from 64.59789 to 64.57058, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6228 - val_loss: 64.5706 - 87ms/epoch - 9ms/step\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 313: val_loss improved from 64.57058 to 64.54366, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6203 - val_loss: 64.5437 - 84ms/epoch - 8ms/step\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 314: val_loss improved from 64.54366 to 64.51714, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6178 - val_loss: 64.5171 - 88ms/epoch - 9ms/step\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 315: val_loss improved from 64.51714 to 64.49100, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6153 - val_loss: 64.4910 - 85ms/epoch - 8ms/step\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 316: val_loss improved from 64.49100 to 64.46513, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6128 - val_loss: 64.4651 - 82ms/epoch - 8ms/step\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 317: val_loss improved from 64.46513 to 64.43964, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6104 - val_loss: 64.4396 - 84ms/epoch - 8ms/step\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 318: val_loss improved from 64.43964 to 64.41445, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6079 - val_loss: 64.4145 - 85ms/epoch - 8ms/step\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 319: val_loss improved from 64.41445 to 64.38972, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6055 - val_loss: 64.3897 - 82ms/epoch - 8ms/step\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 320: val_loss improved from 64.38972 to 64.36520, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6031 - val_loss: 64.3652 - 84ms/epoch - 8ms/step\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 321: val_loss improved from 64.36520 to 64.34103, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.6007 - val_loss: 64.3410 - 93ms/epoch - 9ms/step\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 322: val_loss improved from 64.34103 to 64.31725, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5984 - val_loss: 64.3173 - 100ms/epoch - 10ms/step\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 323: val_loss improved from 64.31725 to 64.29367, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5960 - val_loss: 64.2937 - 87ms/epoch - 9ms/step\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 324: val_loss improved from 64.29367 to 64.27050, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5937 - val_loss: 64.2705 - 90ms/epoch - 9ms/step\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 325: val_loss improved from 64.27050 to 64.24733, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5914 - val_loss: 64.2473 - 84ms/epoch - 8ms/step\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 326: val_loss improved from 64.24733 to 64.22407, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5892 - val_loss: 64.2241 - 82ms/epoch - 8ms/step\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 327: val_loss improved from 64.22407 to 64.20028, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5874 - val_loss: 64.2003 - 84ms/epoch - 8ms/step\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 328: val_loss improved from 64.20028 to 64.17662, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5864 - val_loss: 64.1766 - 84ms/epoch - 8ms/step\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 329: val_loss improved from 64.17662 to 64.15496, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5857 - val_loss: 64.1550 - 83ms/epoch - 8ms/step\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 330: val_loss improved from 64.15496 to 64.13087, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5827 - val_loss: 64.1309 - 89ms/epoch - 9ms/step\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 331: val_loss improved from 64.13087 to 64.10245, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5833 - val_loss: 64.1024 - 102ms/epoch - 10ms/step\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 332: val_loss improved from 64.10245 to 64.08742, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5843 - val_loss: 64.0874 - 95ms/epoch - 9ms/step\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 333: val_loss improved from 64.08742 to 64.06221, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5839 - val_loss: 64.0622 - 90ms/epoch - 9ms/step\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 334: val_loss improved from 64.06221 to 64.05541, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5768 - val_loss: 64.0554 - 94ms/epoch - 9ms/step\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 335: val_loss improved from 64.05541 to 64.03188, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5699 - val_loss: 64.0319 - 95ms/epoch - 10ms/step\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 336: val_loss improved from 64.03188 to 64.01263, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5659 - val_loss: 64.0126 - 86ms/epoch - 9ms/step\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 337: val_loss improved from 64.01263 to 63.99539, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5641 - val_loss: 63.9954 - 91ms/epoch - 9ms/step\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 338: val_loss improved from 63.99539 to 63.97537, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5618 - val_loss: 63.9754 - 99ms/epoch - 10ms/step\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 339: val_loss improved from 63.97537 to 63.95755, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5596 - val_loss: 63.9576 - 87ms/epoch - 9ms/step\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 340: val_loss improved from 63.95755 to 63.93900, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5575 - val_loss: 63.9390 - 97ms/epoch - 10ms/step\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 341: val_loss improved from 63.93900 to 63.92073, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5554 - val_loss: 63.9207 - 97ms/epoch - 10ms/step\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 342: val_loss improved from 63.92073 to 63.90286, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5532 - val_loss: 63.9029 - 99ms/epoch - 10ms/step\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 343: val_loss improved from 63.90286 to 63.88488, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5511 - val_loss: 63.8849 - 84ms/epoch - 8ms/step\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 344: val_loss improved from 63.88488 to 63.86728, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5490 - val_loss: 63.8673 - 92ms/epoch - 9ms/step\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 345: val_loss improved from 63.86728 to 63.84988, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5470 - val_loss: 63.8499 - 87ms/epoch - 9ms/step\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 346: val_loss improved from 63.84988 to 63.83265, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5449 - val_loss: 63.8327 - 84ms/epoch - 8ms/step\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 347: val_loss improved from 63.83265 to 63.81557, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5428 - val_loss: 63.8156 - 93ms/epoch - 9ms/step\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 348: val_loss improved from 63.81557 to 63.79879, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5407 - val_loss: 63.7988 - 92ms/epoch - 9ms/step\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 349: val_loss improved from 63.79879 to 63.78223, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5387 - val_loss: 63.7822 - 99ms/epoch - 10ms/step\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 350: val_loss improved from 63.78223 to 63.76587, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5367 - val_loss: 63.7659 - 193ms/epoch - 19ms/step\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 351: val_loss improved from 63.76587 to 63.74971, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5346 - val_loss: 63.7497 - 100ms/epoch - 10ms/step\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 352: val_loss improved from 63.74971 to 63.73382, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5326 - val_loss: 63.7338 - 85ms/epoch - 9ms/step\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 353: val_loss improved from 63.73382 to 63.71819, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5307 - val_loss: 63.7182 - 96ms/epoch - 10ms/step\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 354: val_loss improved from 63.71819 to 63.70271, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5286 - val_loss: 63.7027 - 88ms/epoch - 9ms/step\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 355: val_loss improved from 63.70271 to 63.68746, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5267 - val_loss: 63.6875 - 83ms/epoch - 8ms/step\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 356: val_loss improved from 63.68746 to 63.67228, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5247 - val_loss: 63.6723 - 85ms/epoch - 9ms/step\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 357: val_loss improved from 63.67228 to 63.65739, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5228 - val_loss: 63.6574 - 95ms/epoch - 10ms/step\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 358: val_loss improved from 63.65739 to 63.64270, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5208 - val_loss: 63.6427 - 81ms/epoch - 8ms/step\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 359: val_loss improved from 63.64270 to 63.62822, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5189 - val_loss: 63.6282 - 80ms/epoch - 8ms/step\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 360: val_loss improved from 63.62822 to 63.61391, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5169 - val_loss: 63.6139 - 89ms/epoch - 9ms/step\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 361: val_loss improved from 63.61391 to 63.59981, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5150 - val_loss: 63.5998 - 93ms/epoch - 9ms/step\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 362: val_loss improved from 63.59981 to 63.58595, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5131 - val_loss: 63.5859 - 82ms/epoch - 8ms/step\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 363: val_loss improved from 63.58595 to 63.57214, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5112 - val_loss: 63.5721 - 99ms/epoch - 10ms/step\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 364: val_loss improved from 63.57214 to 63.55866, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5094 - val_loss: 63.5587 - 92ms/epoch - 9ms/step\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 365: val_loss improved from 63.55866 to 63.54535, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5075 - val_loss: 63.5453 - 93ms/epoch - 9ms/step\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 366: val_loss improved from 63.54535 to 63.53221, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5056 - val_loss: 63.5322 - 94ms/epoch - 9ms/step\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 367: val_loss improved from 63.53221 to 63.51925, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5037 - val_loss: 63.5193 - 84ms/epoch - 8ms/step\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 368: val_loss improved from 63.51925 to 63.50648, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5019 - val_loss: 63.5065 - 91ms/epoch - 9ms/step\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 369: val_loss improved from 63.50648 to 63.49387, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.5000 - val_loss: 63.4939 - 87ms/epoch - 9ms/step\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 370: val_loss improved from 63.49387 to 63.48145, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4982 - val_loss: 63.4814 - 97ms/epoch - 10ms/step\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 371: val_loss improved from 63.48145 to 63.46919, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4964 - val_loss: 63.4692 - 80ms/epoch - 8ms/step\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 372: val_loss improved from 63.46919 to 63.45708, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4946 - val_loss: 63.4571 - 91ms/epoch - 9ms/step\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 373: val_loss improved from 63.45708 to 63.44523, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4928 - val_loss: 63.4452 - 84ms/epoch - 8ms/step\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 374: val_loss improved from 63.44523 to 63.43346, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4909 - val_loss: 63.4335 - 81ms/epoch - 8ms/step\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 375: val_loss improved from 63.43346 to 63.42200, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4892 - val_loss: 63.4220 - 90ms/epoch - 9ms/step\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 376: val_loss improved from 63.42200 to 63.41045, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4874 - val_loss: 63.4104 - 85ms/epoch - 8ms/step\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 377: val_loss improved from 63.41045 to 63.39952, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4856 - val_loss: 63.3995 - 95ms/epoch - 10ms/step\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 378: val_loss improved from 63.39952 to 63.38794, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4838 - val_loss: 63.3879 - 92ms/epoch - 9ms/step\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 379: val_loss improved from 63.38794 to 63.37786, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4821 - val_loss: 63.3779 - 92ms/epoch - 9ms/step\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 380: val_loss improved from 63.37786 to 63.36497, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4805 - val_loss: 63.3650 - 94ms/epoch - 9ms/step\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 381: val_loss improved from 63.36497 to 63.35572, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4790 - val_loss: 63.3557 - 85ms/epoch - 9ms/step\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 382: val_loss improved from 63.35572 to 63.33615, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4786 - val_loss: 63.3362 - 90ms/epoch - 9ms/step\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 383: val_loss improved from 63.33615 to 63.32808, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4799 - val_loss: 63.3281 - 96ms/epoch - 10ms/step\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 384: val_loss improved from 63.32808 to 63.31342, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4833 - val_loss: 63.3134 - 86ms/epoch - 9ms/step\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 385: val_loss improved from 63.31342 to 63.30700, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4799 - val_loss: 63.3070 - 88ms/epoch - 9ms/step\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 386: val_loss improved from 63.30700 to 63.30101, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4743 - val_loss: 63.3010 - 91ms/epoch - 9ms/step\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 387: val_loss improved from 63.30101 to 63.27287, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4774 - val_loss: 63.2729 - 88ms/epoch - 9ms/step\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 388: val_loss did not improve from 63.27287\n",
            "10/10 - 0s - loss: 27.4759 - val_loss: 63.2904 - 70ms/epoch - 7ms/step\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 389: val_loss improved from 63.27287 to 63.27214, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4682 - val_loss: 63.2721 - 94ms/epoch - 9ms/step\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 390: val_loss improved from 63.27214 to 63.26584, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4623 - val_loss: 63.2658 - 86ms/epoch - 9ms/step\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 391: val_loss improved from 63.26584 to 63.26094, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4612 - val_loss: 63.2609 - 85ms/epoch - 9ms/step\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 392: val_loss improved from 63.26094 to 63.25064, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4593 - val_loss: 63.2506 - 88ms/epoch - 9ms/step\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 393: val_loss improved from 63.25064 to 63.24475, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4576 - val_loss: 63.2448 - 89ms/epoch - 9ms/step\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 394: val_loss improved from 63.24475 to 63.23674, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4558 - val_loss: 63.2367 - 88ms/epoch - 9ms/step\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 395: val_loss improved from 63.23674 to 63.22923, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4542 - val_loss: 63.2292 - 88ms/epoch - 9ms/step\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 396: val_loss improved from 63.22923 to 63.22206, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4525 - val_loss: 63.2221 - 103ms/epoch - 10ms/step\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 397: val_loss improved from 63.22206 to 63.21465, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4509 - val_loss: 63.2146 - 98ms/epoch - 10ms/step\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 398: val_loss improved from 63.21465 to 63.20758, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4492 - val_loss: 63.2076 - 95ms/epoch - 9ms/step\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 399: val_loss improved from 63.20758 to 63.20074, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4475 - val_loss: 63.2007 - 78ms/epoch - 8ms/step\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 400: val_loss improved from 63.20074 to 63.19397, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4459 - val_loss: 63.1940 - 82ms/epoch - 8ms/step\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 401: val_loss improved from 63.19397 to 63.18742, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4442 - val_loss: 63.1874 - 95ms/epoch - 9ms/step\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 402: val_loss improved from 63.18742 to 63.18106, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4425 - val_loss: 63.1811 - 80ms/epoch - 8ms/step\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 403: val_loss improved from 63.18106 to 63.17490, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4409 - val_loss: 63.1749 - 91ms/epoch - 9ms/step\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 404: val_loss improved from 63.17490 to 63.16894, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4392 - val_loss: 63.1689 - 82ms/epoch - 8ms/step\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 405: val_loss improved from 63.16894 to 63.16320, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4375 - val_loss: 63.1632 - 100ms/epoch - 10ms/step\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 406: val_loss improved from 63.16320 to 63.15760, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4358 - val_loss: 63.1576 - 82ms/epoch - 8ms/step\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 407: val_loss improved from 63.15760 to 63.15219, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4342 - val_loss: 63.1522 - 94ms/epoch - 9ms/step\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 408: val_loss improved from 63.15219 to 63.14697, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4325 - val_loss: 63.1470 - 90ms/epoch - 9ms/step\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 409: val_loss improved from 63.14697 to 63.14191, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4308 - val_loss: 63.1419 - 90ms/epoch - 9ms/step\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 410: val_loss improved from 63.14191 to 63.13694, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4291 - val_loss: 63.1369 - 84ms/epoch - 8ms/step\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 411: val_loss improved from 63.13694 to 63.13199, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4274 - val_loss: 63.1320 - 89ms/epoch - 9ms/step\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 412: val_loss improved from 63.13199 to 63.12723, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4258 - val_loss: 63.1272 - 95ms/epoch - 10ms/step\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 413: val_loss improved from 63.12723 to 63.12254, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4241 - val_loss: 63.1225 - 88ms/epoch - 9ms/step\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 414: val_loss improved from 63.12254 to 63.11791, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4224 - val_loss: 63.1179 - 86ms/epoch - 9ms/step\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 415: val_loss improved from 63.11791 to 63.11333, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4208 - val_loss: 63.1133 - 92ms/epoch - 9ms/step\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 416: val_loss improved from 63.11333 to 63.10880, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4192 - val_loss: 63.1088 - 108ms/epoch - 11ms/step\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 417: val_loss improved from 63.10880 to 63.10431, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4176 - val_loss: 63.1043 - 94ms/epoch - 9ms/step\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 418: val_loss improved from 63.10431 to 63.09981, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4160 - val_loss: 63.0998 - 96ms/epoch - 10ms/step\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 419: val_loss improved from 63.09981 to 63.09528, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4144 - val_loss: 63.0953 - 90ms/epoch - 9ms/step\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 420: val_loss improved from 63.09528 to 63.09059, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4128 - val_loss: 63.0906 - 85ms/epoch - 9ms/step\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 421: val_loss improved from 63.09059 to 63.08566, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4113 - val_loss: 63.0857 - 84ms/epoch - 8ms/step\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 422: val_loss improved from 63.08566 to 63.08061, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4097 - val_loss: 63.0806 - 91ms/epoch - 9ms/step\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 423: val_loss improved from 63.08061 to 63.07535, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4081 - val_loss: 63.0754 - 93ms/epoch - 9ms/step\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 424: val_loss improved from 63.07535 to 63.06985, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4066 - val_loss: 63.0699 - 88ms/epoch - 9ms/step\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 425: val_loss improved from 63.06985 to 63.06416, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4050 - val_loss: 63.0642 - 84ms/epoch - 8ms/step\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 426: val_loss improved from 63.06416 to 63.05824, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4035 - val_loss: 63.0582 - 92ms/epoch - 9ms/step\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 427: val_loss improved from 63.05824 to 63.05201, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4019 - val_loss: 63.0520 - 101ms/epoch - 10ms/step\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 428: val_loss improved from 63.05201 to 63.04567, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.4004 - val_loss: 63.0457 - 85ms/epoch - 9ms/step\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 429: val_loss improved from 63.04567 to 63.03916, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3988 - val_loss: 63.0392 - 90ms/epoch - 9ms/step\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 430: val_loss improved from 63.03916 to 63.03243, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3973 - val_loss: 63.0324 - 92ms/epoch - 9ms/step\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 431: val_loss improved from 63.03243 to 63.02553, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3958 - val_loss: 63.0255 - 94ms/epoch - 9ms/step\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 432: val_loss improved from 63.02553 to 63.01843, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3943 - val_loss: 63.0184 - 89ms/epoch - 9ms/step\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 433: val_loss improved from 63.01843 to 63.01118, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3927 - val_loss: 63.0112 - 84ms/epoch - 8ms/step\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 434: val_loss improved from 63.01118 to 63.00364, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3913 - val_loss: 63.0036 - 83ms/epoch - 8ms/step\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 435: val_loss improved from 63.00364 to 62.99646, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3897 - val_loss: 62.9965 - 88ms/epoch - 9ms/step\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 436: val_loss improved from 62.99646 to 62.98819, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3883 - val_loss: 62.9882 - 95ms/epoch - 9ms/step\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 437: val_loss improved from 62.98819 to 62.98149, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3868 - val_loss: 62.9815 - 97ms/epoch - 10ms/step\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 438: val_loss improved from 62.98149 to 62.97119, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3855 - val_loss: 62.9712 - 101ms/epoch - 10ms/step\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 439: val_loss improved from 62.97119 to 62.96578, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3841 - val_loss: 62.9658 - 94ms/epoch - 9ms/step\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 440: val_loss improved from 62.96578 to 62.94655, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3840 - val_loss: 62.9465 - 85ms/epoch - 9ms/step\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 441: val_loss improved from 62.94655 to 62.94226, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3851 - val_loss: 62.9423 - 97ms/epoch - 10ms/step\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 442: val_loss improved from 62.94226 to 62.92064, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3913 - val_loss: 62.9206 - 88ms/epoch - 9ms/step\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 443: val_loss did not improve from 62.92064\n",
            "10/10 - 0s - loss: 27.3901 - val_loss: 62.9253 - 62ms/epoch - 6ms/step\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 444: val_loss improved from 62.92064 to 62.91849, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3818 - val_loss: 62.9185 - 84ms/epoch - 8ms/step\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 445: val_loss improved from 62.91849 to 62.88531, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3837 - val_loss: 62.8853 - 94ms/epoch - 9ms/step\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 446: val_loss did not improve from 62.88531\n",
            "10/10 - 0s - loss: 27.3853 - val_loss: 62.9118 - 70ms/epoch - 7ms/step\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 447: val_loss did not improve from 62.88531\n",
            "10/10 - 0s - loss: 27.3785 - val_loss: 62.8903 - 64ms/epoch - 6ms/step\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 448: val_loss did not improve from 62.88531\n",
            "10/10 - 0s - loss: 27.3704 - val_loss: 62.8874 - 58ms/epoch - 6ms/step\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 449: val_loss improved from 62.88531 to 62.88420, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3697 - val_loss: 62.8842 - 92ms/epoch - 9ms/step\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 450: val_loss improved from 62.88420 to 62.87386, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3680 - val_loss: 62.8739 - 97ms/epoch - 10ms/step\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 451: val_loss improved from 62.87386 to 62.86950, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3666 - val_loss: 62.8695 - 88ms/epoch - 9ms/step\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 452: val_loss improved from 62.86950 to 62.86164, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3652 - val_loss: 62.8616 - 97ms/epoch - 10ms/step\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 453: val_loss improved from 62.86164 to 62.85439, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3639 - val_loss: 62.8544 - 96ms/epoch - 10ms/step\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 454: val_loss improved from 62.85439 to 62.84726, saving model to co2.h5\n",
            "10/10 - 1s - loss: 27.3624 - val_loss: 62.8473 - 509ms/epoch - 51ms/step\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 455: val_loss improved from 62.84726 to 62.83967, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3612 - val_loss: 62.8397 - 88ms/epoch - 9ms/step\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 456: val_loss improved from 62.83967 to 62.83228, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3598 - val_loss: 62.8323 - 96ms/epoch - 10ms/step\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 457: val_loss improved from 62.83228 to 62.82500, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3585 - val_loss: 62.8250 - 92ms/epoch - 9ms/step\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 458: val_loss improved from 62.82500 to 62.81757, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3572 - val_loss: 62.8176 - 95ms/epoch - 9ms/step\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 459: val_loss improved from 62.81757 to 62.81042, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3559 - val_loss: 62.8104 - 85ms/epoch - 9ms/step\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 460: val_loss improved from 62.81042 to 62.80322, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3545 - val_loss: 62.8032 - 102ms/epoch - 10ms/step\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 461: val_loss improved from 62.80322 to 62.79623, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3533 - val_loss: 62.7962 - 89ms/epoch - 9ms/step\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 462: val_loss improved from 62.79623 to 62.78932, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3520 - val_loss: 62.7893 - 95ms/epoch - 10ms/step\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 463: val_loss improved from 62.78932 to 62.78260, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3507 - val_loss: 62.7826 - 89ms/epoch - 9ms/step\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 464: val_loss improved from 62.78260 to 62.77600, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3494 - val_loss: 62.7760 - 94ms/epoch - 9ms/step\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 465: val_loss improved from 62.77600 to 62.76956, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3482 - val_loss: 62.7696 - 92ms/epoch - 9ms/step\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 466: val_loss improved from 62.76956 to 62.76323, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3469 - val_loss: 62.7632 - 94ms/epoch - 9ms/step\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 467: val_loss improved from 62.76323 to 62.75708, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3457 - val_loss: 62.7571 - 101ms/epoch - 10ms/step\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 468: val_loss improved from 62.75708 to 62.75105, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3444 - val_loss: 62.7511 - 86ms/epoch - 9ms/step\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 469: val_loss improved from 62.75105 to 62.74522, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3432 - val_loss: 62.7452 - 83ms/epoch - 8ms/step\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 470: val_loss improved from 62.74522 to 62.73941, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3419 - val_loss: 62.7394 - 96ms/epoch - 10ms/step\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 471: val_loss improved from 62.73941 to 62.73390, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3407 - val_loss: 62.7339 - 90ms/epoch - 9ms/step\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 472: val_loss improved from 62.73390 to 62.72855, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3395 - val_loss: 62.7285 - 91ms/epoch - 9ms/step\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 473: val_loss improved from 62.72855 to 62.72341, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3383 - val_loss: 62.7234 - 87ms/epoch - 9ms/step\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 474: val_loss improved from 62.72341 to 62.71852, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3370 - val_loss: 62.7185 - 89ms/epoch - 9ms/step\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 475: val_loss improved from 62.71852 to 62.71385, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3358 - val_loss: 62.7138 - 82ms/epoch - 8ms/step\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 476: val_loss improved from 62.71385 to 62.70945, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3346 - val_loss: 62.7095 - 89ms/epoch - 9ms/step\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 477: val_loss improved from 62.70945 to 62.70528, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3334 - val_loss: 62.7053 - 88ms/epoch - 9ms/step\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 478: val_loss improved from 62.70528 to 62.70145, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3322 - val_loss: 62.7014 - 94ms/epoch - 9ms/step\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 479: val_loss improved from 62.70145 to 62.69789, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3310 - val_loss: 62.6979 - 95ms/epoch - 9ms/step\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 480: val_loss improved from 62.69789 to 62.69463, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3298 - val_loss: 62.6946 - 86ms/epoch - 9ms/step\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 481: val_loss improved from 62.69463 to 62.69168, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3286 - val_loss: 62.6917 - 84ms/epoch - 8ms/step\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 482: val_loss improved from 62.69168 to 62.68906, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3274 - val_loss: 62.6891 - 87ms/epoch - 9ms/step\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 483: val_loss improved from 62.68906 to 62.68679, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3262 - val_loss: 62.6868 - 87ms/epoch - 9ms/step\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 484: val_loss improved from 62.68679 to 62.68483, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3250 - val_loss: 62.6848 - 96ms/epoch - 10ms/step\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 485: val_loss improved from 62.68483 to 62.68323, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3238 - val_loss: 62.6832 - 95ms/epoch - 10ms/step\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 486: val_loss improved from 62.68323 to 62.68199, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3226 - val_loss: 62.6820 - 97ms/epoch - 10ms/step\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 487: val_loss improved from 62.68199 to 62.68107, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3214 - val_loss: 62.6811 - 95ms/epoch - 9ms/step\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 488: val_loss improved from 62.68107 to 62.68047, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3202 - val_loss: 62.6805 - 105ms/epoch - 10ms/step\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 489: val_loss improved from 62.68047 to 62.68019, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3190 - val_loss: 62.6802 - 88ms/epoch - 9ms/step\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 490: val_loss improved from 62.68019 to 62.68007, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3178 - val_loss: 62.6801 - 89ms/epoch - 9ms/step\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 491: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3166 - val_loss: 62.6804 - 71ms/epoch - 7ms/step\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 492: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3154 - val_loss: 62.6808 - 60ms/epoch - 6ms/step\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 493: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3142 - val_loss: 62.6817 - 58ms/epoch - 6ms/step\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 494: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3130 - val_loss: 62.6823 - 78ms/epoch - 8ms/step\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 495: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3118 - val_loss: 62.6837 - 60ms/epoch - 6ms/step\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 496: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3107 - val_loss: 62.6838 - 62ms/epoch - 6ms/step\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 497: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3095 - val_loss: 62.6862 - 64ms/epoch - 6ms/step\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 498: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3084 - val_loss: 62.6845 - 71ms/epoch - 7ms/step\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 499: val_loss did not improve from 62.68007\n",
            "10/10 - 0s - loss: 27.3074 - val_loss: 62.6888 - 67ms/epoch - 7ms/step\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 500: val_loss improved from 62.68007 to 62.67940, saving model to co2.h5\n",
            "10/10 - 0s - loss: 27.3070 - val_loss: 62.6794 - 86ms/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(path)"
      ],
      "metadata": {
        "id": "qcOW4kSX3dG0"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_X)\n",
        "real = YTest\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fdca7d-aa61-49e3-d680-cf296179b813",
        "id": "KUVhf5o13dG1"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[83.35397 ],\n",
              "       [85.28352 ],\n",
              "       [86.723404],\n",
              "       [86.75409 ],\n",
              "       [83.247116],\n",
              "       [84.50447 ],\n",
              "       [84.45995 ],\n",
              "       [86.48218 ],\n",
              "       [86.502625],\n",
              "       [86.260124],\n",
              "       [80.9     ],\n",
              "       [82.90154 ],\n",
              "       [86.08233 ],\n",
              "       [86.61322 ],\n",
              "       [85.591034],\n",
              "       [85.94348 ],\n",
              "       [86.50822 ],\n",
              "       [86.604225],\n",
              "       [84.31922 ],\n",
              "       [85.461494],\n",
              "       [86.92114 ],\n",
              "       [86.60423 ],\n",
              "       [86.451256],\n",
              "       [86.22561 ],\n",
              "       [86.77252 ],\n",
              "       [86.54613 ],\n",
              "       [84.41854 ],\n",
              "       [85.901985],\n",
              "       [86.466   ],\n",
              "       [86.67798 ],\n",
              "       [86.07715 ],\n",
              "       [86.55163 ],\n",
              "       [85.81262 ],\n",
              "       [86.34596 ],\n",
              "       [84.456345],\n",
              "       [85.88512 ],\n",
              "       [86.978195],\n",
              "       [86.39854 ],\n",
              "       [86.842384],\n",
              "       [86.54272 ],\n",
              "       [86.47224 ],\n",
              "       [85.205635],\n",
              "       [86.880585],\n",
              "       [85.61694 ],\n",
              "       [86.02511 ],\n",
              "       [86.1521  ],\n",
              "       [71.808624],\n",
              "       [72.06874 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE CO2: {}'.format(numpy.sqrt(numpy.mean((pred[:, 0]-real[:, 0])**2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6b50ec-9862-4ae5-d644-9b4e346d0269",
        "id": "NwXic3qU3dG2"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE CO2: 7.9170327132079725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Debu"
      ],
      "metadata": {
        "id": "0uZXHlZW965W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset_debu.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "c1k9_ot7965W"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1adf8d8e-8e4b-416d-f48d-4257a5339dd3",
        "id": "U06VX-ko965X"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 416 entries, 0 to 415\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   PM10    416 non-null    float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 3.4 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(len(df)*.6)\n",
        "dfTrain = df.iloc[:split, :]\n",
        "dfTest = df.iloc[split:, :]"
      ],
      "metadata": {
        "id": "BgnK4ouH965X"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deret_waktu(data, n_in=1, n_out=1, dropnan=True,):\n",
        "    n_vars = (1 if type(data) is list else data.shape[1])\n",
        "    df = DataFrame(data)\n",
        "    (cols, names) = (list(), list())\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += ['var%d(t-%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += ['var%d(t)' % (j + 1) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += ['var%d(t+%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "07kK_-Qq965X"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag = 2\n",
        "output = 1\n",
        "reframedTrain = deret_waktu(dfTrain, lag, output)\n",
        "reframedTest = deret_waktu(dfTest, lag, output)"
      ],
      "metadata": {
        "id": "a1gcC4Xs965X"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframedTest.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "917e5125-8d31-4613-aefa-2ebd981ce323",
        "id": "OPxqd8J5965X"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     var1(t-2)  var1(t-1)    var1(t)\n",
              "251  10.600119  13.364220  11.024105\n",
              "252  13.364220  11.024105  11.453035\n",
              "253  11.024105  11.453035  15.023557\n",
              "254  11.453035  15.023557  10.410371\n",
              "255  15.023557  10.410371  11.209158"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d463a15c-b155-4710-9f0a-1c0b2e943c31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-2)</th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>10.600119</td>\n",
              "      <td>13.364220</td>\n",
              "      <td>11.024105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>13.364220</td>\n",
              "      <td>11.024105</td>\n",
              "      <td>11.453035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>11.024105</td>\n",
              "      <td>11.453035</td>\n",
              "      <td>15.023557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>11.453035</td>\n",
              "      <td>15.023557</td>\n",
              "      <td>10.410371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>15.023557</td>\n",
              "      <td>10.410371</td>\n",
              "      <td>11.209158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d463a15c-b155-4710-9f0a-1c0b2e943c31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d463a15c-b155-4710-9f0a-1c0b2e943c31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d463a15c-b155-4710-9f0a-1c0b2e943c31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = len(df.columns)\n",
        "YTrain = reframedTrain.iloc[:, -n_features:].values\n",
        "YTest = reframedTest.iloc[:, -n_features:].values\n",
        "n_obs = lag * n_features\n",
        "train_X, train_y = reframedTrain.iloc[:, :n_obs].values, YTrain\n",
        "test_X, test_y = reframedTest.iloc[:, :n_obs].values, YTest\n",
        "train_X = train_X.reshape((train_X.shape[0], lag, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], lag, n_features))"
      ],
      "metadata": {
        "id": "OQA9J28T965Y"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(train_X.shape[-2:])))\n",
        "model.add(Dense(n_features, activation=\"relu\"))\n",
        "model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              loss='mse')  \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b15bf3-bd40-47d0-a719-1d61368dd3ec",
        "id": "mmOeQzly965Y"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_15 (LSTM)              (None, 256)               264192    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 264,449\n",
            "Trainable params: 264,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'debu.h5'\n",
        "modelckpt_callback = tensorflow.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
        "                                                     filepath=path, \n",
        "                                                     verbose=1, \n",
        "                                                     save_weights_only=False, \n",
        "                                                     save_best_only=True)\n",
        "es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                            min_delta=0, patience=10)\n",
        "history = model.fit(train_X, train_y, epochs=500, batch_size=8, \n",
        "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
        "                    callbacks=[modelckpt_callback, es_callback])\n",
        "\n",
        "loss = history.history['loss']\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02ed004a-51f4-489f-f6b3-8f74e238037d",
        "id": "1uIki3G7965Y"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.50895, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.4225 - val_loss: 2.5090 - 250ms/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 2: val_loss improved from 2.50895 to 2.36881, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.5001 - val_loss: 2.3688 - 278ms/epoch - 9ms/step\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.36881\n",
            "31/31 - 0s - loss: 2.4375 - val_loss: 2.5349 - 244ms/epoch - 8ms/step\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 4: val_loss improved from 2.36881 to 2.35065, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.4850 - val_loss: 2.3507 - 342ms/epoch - 11ms/step\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.35065\n",
            "31/31 - 0s - loss: 2.3986 - val_loss: 2.4810 - 239ms/epoch - 8ms/step\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 6: val_loss improved from 2.35065 to 2.25493, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.4128 - val_loss: 2.2549 - 400ms/epoch - 13ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.25493\n",
            "31/31 - 0s - loss: 2.3154 - val_loss: 2.3419 - 224ms/epoch - 7ms/step\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 8: val_loss improved from 2.25493 to 2.22242, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.3282 - val_loss: 2.2224 - 258ms/epoch - 8ms/step\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.22242\n",
            "31/31 - 0s - loss: 2.2764 - val_loss: 2.2632 - 251ms/epoch - 8ms/step\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 10: val_loss improved from 2.22242 to 2.21366, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2858 - val_loss: 2.2137 - 268ms/epoch - 9ms/step\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.21366\n",
            "31/31 - 0s - loss: 2.2631 - val_loss: 2.2348 - 239ms/epoch - 8ms/step\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 12: val_loss improved from 2.21366 to 2.21320, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2693 - val_loss: 2.2132 - 263ms/epoch - 8ms/step\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.21320\n",
            "31/31 - 0s - loss: 2.2587 - val_loss: 2.2251 - 228ms/epoch - 7ms/step\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 14: val_loss improved from 2.21320 to 2.21302, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2625 - val_loss: 2.2130 - 291ms/epoch - 9ms/step\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.21302\n",
            "31/31 - 0s - loss: 2.2559 - val_loss: 2.2205 - 247ms/epoch - 8ms/step\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 16: val_loss improved from 2.21302 to 2.21224, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2583 - val_loss: 2.2122 - 269ms/epoch - 9ms/step\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.21224\n",
            "31/31 - 0s - loss: 2.2533 - val_loss: 2.2176 - 324ms/epoch - 10ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: val_loss improved from 2.21224 to 2.21117, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2551 - val_loss: 2.2112 - 296ms/epoch - 10ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.21117\n",
            "31/31 - 0s - loss: 2.2508 - val_loss: 2.2155 - 319ms/epoch - 10ms/step\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 20: val_loss improved from 2.21117 to 2.21004, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2524 - val_loss: 2.2100 - 453ms/epoch - 15ms/step\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.21004\n",
            "31/31 - 0s - loss: 2.2485 - val_loss: 2.2140 - 148ms/epoch - 5ms/step\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 22: val_loss improved from 2.21004 to 2.20896, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2501 - val_loss: 2.2090 - 180ms/epoch - 6ms/step\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.20896\n",
            "31/31 - 0s - loss: 2.2461 - val_loss: 2.2129 - 158ms/epoch - 5ms/step\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 24: val_loss improved from 2.20896 to 2.20797, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2482 - val_loss: 2.2080 - 175ms/epoch - 6ms/step\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.20797\n",
            "31/31 - 0s - loss: 2.2439 - val_loss: 2.2122 - 153ms/epoch - 5ms/step\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 26: val_loss improved from 2.20797 to 2.20713, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2466 - val_loss: 2.2071 - 200ms/epoch - 6ms/step\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.20713\n",
            "31/31 - 0s - loss: 2.2418 - val_loss: 2.2119 - 176ms/epoch - 6ms/step\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 28: val_loss improved from 2.20713 to 2.20648, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2455 - val_loss: 2.2065 - 185ms/epoch - 6ms/step\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.20648\n",
            "31/31 - 0s - loss: 2.2397 - val_loss: 2.2120 - 170ms/epoch - 5ms/step\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 30: val_loss improved from 2.20648 to 2.20614, saving model to debu.h5\n",
            "31/31 - 0s - loss: 2.2450 - val_loss: 2.2061 - 168ms/epoch - 5ms/step\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2376 - val_loss: 2.2128 - 161ms/epoch - 5ms/step\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2455 - val_loss: 2.2064 - 181ms/epoch - 6ms/step\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2359 - val_loss: 2.2146 - 145ms/epoch - 5ms/step\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2476 - val_loss: 2.2080 - 169ms/epoch - 5ms/step\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2349 - val_loss: 2.2186 - 172ms/epoch - 6ms/step\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2530 - val_loss: 2.2121 - 174ms/epoch - 6ms/step\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2360 - val_loss: 2.2267 - 170ms/epoch - 5ms/step\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2640 - val_loss: 2.2210 - 164ms/epoch - 5ms/step\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2416 - val_loss: 2.2427 - 151ms/epoch - 5ms/step\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 40: val_loss did not improve from 2.20614\n",
            "31/31 - 0s - loss: 2.2845 - val_loss: 2.2367 - 156ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(path)"
      ],
      "metadata": {
        "id": "pq5lUsCu965Y"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_X)\n",
        "real = YTest\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176f7c60-f4f5-4c78-c6aa-bb3c3336de09",
        "id": "V42OtImS965Y"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 59 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffa179b9cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13.53678  ],\n",
              "       [10.924589 ],\n",
              "       [12.114279 ],\n",
              "       [14.608066 ],\n",
              "       [ 9.005384 ],\n",
              "       [11.90324  ],\n",
              "       [10.322703 ],\n",
              "       [11.990847 ],\n",
              "       [11.194369 ],\n",
              "       [11.873602 ],\n",
              "       [10.860042 ],\n",
              "       [ 9.442156 ],\n",
              "       [ 7.464162 ],\n",
              "       [ 8.072557 ],\n",
              "       [ 7.6561966],\n",
              "       [ 7.167755 ],\n",
              "       [ 6.714023 ],\n",
              "       [ 7.9389496],\n",
              "       [ 8.351317 ],\n",
              "       [10.441409 ],\n",
              "       [ 8.074084 ],\n",
              "       [ 9.832464 ],\n",
              "       [10.048638 ],\n",
              "       [ 8.855127 ],\n",
              "       [ 8.7864   ],\n",
              "       [ 7.7769227],\n",
              "       [ 8.365524 ],\n",
              "       [ 8.771313 ],\n",
              "       [ 8.665756 ],\n",
              "       [ 8.478107 ],\n",
              "       [ 9.571525 ],\n",
              "       [11.130422 ],\n",
              "       [ 9.776917 ],\n",
              "       [10.822242 ],\n",
              "       [10.142309 ],\n",
              "       [10.555725 ],\n",
              "       [ 8.08934  ],\n",
              "       [ 8.088608 ],\n",
              "       [ 8.063415 ],\n",
              "       [ 9.344754 ],\n",
              "       [ 9.063522 ],\n",
              "       [ 8.591379 ],\n",
              "       [ 9.934926 ],\n",
              "       [10.460099 ],\n",
              "       [ 9.342842 ],\n",
              "       [ 8.496397 ],\n",
              "       [ 9.942791 ],\n",
              "       [ 9.768298 ],\n",
              "       [ 9.861053 ],\n",
              "       [ 8.922246 ],\n",
              "       [ 9.221271 ],\n",
              "       [ 9.490473 ],\n",
              "       [ 8.627649 ],\n",
              "       [ 8.095467 ],\n",
              "       [ 8.115297 ],\n",
              "       [10.157219 ],\n",
              "       [10.907831 ],\n",
              "       [ 9.054544 ],\n",
              "       [ 9.681156 ],\n",
              "       [11.513159 ],\n",
              "       [ 9.281322 ],\n",
              "       [ 8.410186 ],\n",
              "       [ 9.448364 ],\n",
              "       [ 9.395196 ],\n",
              "       [ 9.236267 ],\n",
              "       [10.292905 ],\n",
              "       [ 9.7673   ],\n",
              "       [10.056298 ],\n",
              "       [11.673182 ],\n",
              "       [12.162305 ],\n",
              "       [12.702456 ],\n",
              "       [12.350691 ],\n",
              "       [12.480243 ],\n",
              "       [12.105416 ],\n",
              "       [11.914894 ],\n",
              "       [12.321821 ],\n",
              "       [11.457801 ],\n",
              "       [11.293065 ],\n",
              "       [12.078633 ],\n",
              "       [12.117833 ],\n",
              "       [12.974722 ],\n",
              "       [14.242512 ],\n",
              "       [14.016829 ],\n",
              "       [13.610685 ],\n",
              "       [14.273408 ],\n",
              "       [14.668856 ],\n",
              "       [16.77917  ],\n",
              "       [10.938932 ],\n",
              "       [ 6.7359414],\n",
              "       [ 9.205886 ],\n",
              "       [10.415391 ],\n",
              "       [11.296534 ],\n",
              "       [14.760176 ],\n",
              "       [17.158981 ],\n",
              "       [16.131655 ],\n",
              "       [16.834757 ],\n",
              "       [15.901415 ],\n",
              "       [16.672085 ],\n",
              "       [16.215042 ],\n",
              "       [16.109781 ],\n",
              "       [17.409872 ],\n",
              "       [17.38874  ],\n",
              "       [16.036383 ],\n",
              "       [14.352815 ],\n",
              "       [15.733374 ],\n",
              "       [16.414507 ],\n",
              "       [16.669155 ],\n",
              "       [17.022379 ],\n",
              "       [17.851118 ],\n",
              "       [14.273011 ],\n",
              "       [14.797329 ],\n",
              "       [14.080772 ],\n",
              "       [13.20311  ],\n",
              "       [13.997187 ],\n",
              "       [14.196307 ],\n",
              "       [13.320017 ],\n",
              "       [13.762253 ],\n",
              "       [ 6.7909255],\n",
              "       [ 5.6694546],\n",
              "       [ 6.182997 ],\n",
              "       [ 7.24216  ],\n",
              "       [ 6.765654 ],\n",
              "       [ 4.878015 ],\n",
              "       [ 5.647804 ],\n",
              "       [ 5.5556803],\n",
              "       [ 5.3167057],\n",
              "       [ 4.931843 ],\n",
              "       [ 4.459084 ],\n",
              "       [ 4.099438 ],\n",
              "       [ 4.03795  ],\n",
              "       [ 3.8895826],\n",
              "       [ 3.670236 ],\n",
              "       [ 3.7589564],\n",
              "       [ 4.4269767],\n",
              "       [ 4.5654006],\n",
              "       [ 5.402504 ],\n",
              "       [ 6.5783157],\n",
              "       [ 7.1518106],\n",
              "       [ 6.237781 ],\n",
              "       [ 6.7297797],\n",
              "       [ 5.381423 ],\n",
              "       [ 7.285652 ],\n",
              "       [ 7.623679 ],\n",
              "       [ 2.3979838],\n",
              "       [ 3.6075857],\n",
              "       [ 3.4936993],\n",
              "       [ 3.95291  ],\n",
              "       [ 4.232138 ],\n",
              "       [ 4.1636133],\n",
              "       [ 4.037338 ],\n",
              "       [ 4.309643 ],\n",
              "       [ 4.188121 ],\n",
              "       [ 4.4831595],\n",
              "       [ 4.0209174],\n",
              "       [ 4.1634564],\n",
              "       [ 4.2721663],\n",
              "       [ 4.280495 ],\n",
              "       [ 4.838819 ],\n",
              "       [ 5.052038 ],\n",
              "       [ 4.5690336],\n",
              "       [ 5.686914 ],\n",
              "       [ 7.1773477],\n",
              "       [ 7.579356 ],\n",
              "       [ 8.182065 ],\n",
              "       [ 6.014729 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE Debu: {}'.format(numpy.sqrt(numpy.mean((pred[:, 0]-real[:, 0])**2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5ac288-cdb3-4bba-fccb-933295429c9a",
        "id": "oNLR6L4g965Z"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Debu: 1.4853085406429754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tekanan Udara"
      ],
      "metadata": {
        "id": "BMgu54Bv-wo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset_tekanan_udara.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "yulsEb4q-wo3"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c737e47-51d1-454f-9c5f-7c6808e13539",
        "id": "wDwreTRi-wo3"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3654 entries, 0 to 3653\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Pressure  3654 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 28.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(len(df)*.6)\n",
        "dfTrain = df.iloc[:split, :]\n",
        "dfTest = df.iloc[split:, :]"
      ],
      "metadata": {
        "id": "DsSNb3Tv-wo3"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deret_waktu(data, n_in=1, n_out=1, dropnan=True,):\n",
        "    n_vars = (1 if type(data) is list else data.shape[1])\n",
        "    df = DataFrame(data)\n",
        "    (cols, names) = (list(), list())\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += ['var%d(t-%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += ['var%d(t)' % (j + 1) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += ['var%d(t+%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "sfQo0lhv-wo3"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag = 2\n",
        "output = 1\n",
        "reframedTrain = deret_waktu(dfTrain, lag, output)\n",
        "reframedTest = deret_waktu(dfTest, lag, output)"
      ],
      "metadata": {
        "id": "Yzlr4svR-wo4"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframedTest.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c0ed5d01-722a-4da8-d64d-4efb4c4aed8e",
        "id": "eWE6PHL_-wo4"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      var1(t-2)  var1(t-1)  var1(t)\n",
              "2194     1.0076     1.0187   1.0285\n",
              "2195     1.0187     1.0285   1.0258\n",
              "2196     1.0285     1.0258   1.0179\n",
              "2197     1.0258     1.0179   1.0209\n",
              "2198     1.0179     1.0209   1.0263"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b43783e6-9e44-4fd0-8fb9-18c36255f293\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-2)</th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2194</th>\n",
              "      <td>1.0076</td>\n",
              "      <td>1.0187</td>\n",
              "      <td>1.0285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2195</th>\n",
              "      <td>1.0187</td>\n",
              "      <td>1.0285</td>\n",
              "      <td>1.0258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2196</th>\n",
              "      <td>1.0285</td>\n",
              "      <td>1.0258</td>\n",
              "      <td>1.0179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2197</th>\n",
              "      <td>1.0258</td>\n",
              "      <td>1.0179</td>\n",
              "      <td>1.0209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2198</th>\n",
              "      <td>1.0179</td>\n",
              "      <td>1.0209</td>\n",
              "      <td>1.0263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b43783e6-9e44-4fd0-8fb9-18c36255f293')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b43783e6-9e44-4fd0-8fb9-18c36255f293 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b43783e6-9e44-4fd0-8fb9-18c36255f293');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = len(df.columns)\n",
        "YTrain = reframedTrain.iloc[:, -n_features:].values\n",
        "YTest = reframedTest.iloc[:, -n_features:].values\n",
        "n_obs = lag * n_features\n",
        "train_X, train_y = reframedTrain.iloc[:, :n_obs].values, YTrain\n",
        "test_X, test_y = reframedTest.iloc[:, :n_obs].values, YTest\n",
        "train_X = train_X.reshape((train_X.shape[0], lag, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], lag, n_features))"
      ],
      "metadata": {
        "id": "A7PxvIfQ-wo4"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape=(train_X.shape[-2:])))\n",
        "model.add(Dense(n_features, activation=\"relu\"))\n",
        "model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              loss='mse')  \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61d3363-a1e0-4a78-8c7e-61e1de290481",
        "id": "cbQBBXHK-wo4"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_17 (LSTM)              (None, 512)               1052672   \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,053,185\n",
            "Trainable params: 1,053,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'tekanan.h5'\n",
        "modelckpt_callback = tensorflow.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
        "                                                     filepath=path, \n",
        "                                                     verbose=1, \n",
        "                                                     save_weights_only=False, \n",
        "                                                     save_best_only=True)\n",
        "es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                            min_delta=0, patience=10)\n",
        "history = model.fit(train_X, train_y, epochs=500, batch_size=8, \n",
        "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
        "                    callbacks=[modelckpt_callback, es_callback])\n",
        "\n",
        "loss = history.history['loss']\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89ae4f50-134d-40fe-bcad-3aa583ab6e2d",
        "id": "daXPELpo-wo4"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to tekanan.h5\n",
            "274/274 - 4s - loss: 0.0244 - val_loss: 3.6386e-05 - 4s/epoch - 14ms/step\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 3.9116e-05 - val_loss: 3.6888e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "274/274 - 2s - loss: 4.0407e-05 - val_loss: 3.6676e-05 - 2s/epoch - 5ms/step\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 4: val_loss improved from 0.00004 to 0.00004, saving model to tekanan.h5\n",
            "274/274 - 2s - loss: 4.1538e-05 - val_loss: 3.6337e-05 - 2s/epoch - 6ms/step\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 4.2485e-05 - val_loss: 3.7283e-05 - 1s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 4.2802e-05 - val_loss: 3.7681e-05 - 1s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: val_loss improved from 0.00004 to 0.00004, saving model to tekanan.h5\n",
            "274/274 - 1s - loss: 4.3277e-05 - val_loss: 3.6248e-05 - 1s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 4.5899e-05 - val_loss: 4.4206e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00004, saving model to tekanan.h5\n",
            "274/274 - 1s - loss: 4.9109e-05 - val_loss: 3.6103e-05 - 1s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 5.0219e-05 - val_loss: 4.3167e-05 - 1s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 5.1799e-05 - val_loss: 6.4918e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 5.1691e-05 - val_loss: 7.1684e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 5.3383e-05 - val_loss: 8.5668e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 5.8033e-05 - val_loss: 9.8338e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 6.3306e-05 - val_loss: 9.6918e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 6.9335e-05 - val_loss: 9.0032e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 7.5142e-05 - val_loss: 8.7707e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 7.8190e-05 - val_loss: 8.9935e-05 - 1s/epoch - 5ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.00004\n",
            "274/274 - 1s - loss: 7.9225e-05 - val_loss: 9.4897e-05 - 1s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAHiCAYAAADoA5FMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfbRlZX0n+O+PKl7lTaBArDon4IgaFSikEJVIUJMokQgajNAukSHxLTGJGmNIbIVl4qxOh5l2nGi6MUaNYxpsM9KkxSYxSNAYDQVhNChM0CApQITiHSyg4Jk/zrnlrVv3Vt2quveet89nrbvOPns/e5/fPrvuy7eefZ6nWmsBAACAYbXLoAsAAACArRFcAQAAGGqCKwAAAENNcAUAAGCoCa4AAAAMNcEVAACAoSa4AjDUquqLVfXGhW47SFV1c1X9zCIct1XV0/vL/7mq3jeftjvwOq+vqr/e0Tq3ctyTqmrdQh8XgNG3fNAFADB+qurBaU/3SvJIksf7z9/SWvvMfI/VWjt5MdqOu9baWxfiOFV1WJJ/TbJra21j/9ifSTLvawgAO0twBWDBtdb2nlquqpuT/Epr7Usz21XV8qkwBAAwF7cKA7Bkpm4FrarfqaofJPlEVT25qv5HVd1ZVff0l1dN2+fKqvqV/vLZVfXVqrqg3/Zfq+rkHWx7eFVdVVUPVNWXquojVfV/z1H3fGr8/ar6+/7x/rqqDpq2/Q1V9f2qWl9V793K+3N8Vf2gqpZNW/fqqvpmf/n5VfUPVXVvVd1eVX9cVbvNcaxPVtUfTHv+2/19bquqc2a0fWVV/VNV3V9V/1ZV50/bfFX/8d6qerCqXjj13k7b/0VVdXVV3dd/fNF835utqaqf7O9/b1VdX1Wvmrbt56vq2/1j3lpV7+6vP6h/fe6tqrur6itV5e8dgBHnBzkAS+0pSQ5I8hNJ3pze76JP9J93k/woyR9vZf/jk9yY5KAk/zHJx6uqdqDtXyT5xyQHJjk/yRu28przqfHfJflfkxycZLckU0Hq2Un+pH/8p/Zfb1Vm0Vr7RpKHkrx0xnH/or/8eJJ39s/nhUleluRXt1J3+jW8ol/PzyY5IsnMz9c+lOSsJPsneWWSt1XVaf1tJ/Yf92+t7d1a+4cZxz4gyReSfLh/bv9Hki9U1YEzzmGL92YbNe+a5K+S/HV/v19P8pmqema/ycfTu+18nyTPTXJFf/1vJVmXZEWSQ5L8XpK2rdcDYLgJrgAstSeSnNdae6S19qPW2vrW2l+21h5urT2Q5INJfnor+3+/tfax1trjST6V5ND0Asq821ZVN8lxSd7fWnu0tfbVJJfO9YLzrPETrbX/r7X2oySfTbK6v/70JP+jtXZVa+2RJO/rvwdz+a9JzkySqtonyc/316W1dk1r7euttY2ttZuT/JdZ6pjNL/Xr++fW2kPpBfXp53dla+1brbUnWmvf7L/efI6b9ILuv7TWPt2v678muSHJL0xrM9d7szUvSLJ3kv/Qv0ZXJPkf6b83SR5L8uyq2re1dk9r7dpp6w9N8hOttcdaa19prQmuACNOcAVgqd3ZWtsw9aSq9qqq/9K/lfb+9G5N3X/67bIz/GBqobX2cH9x7+1s+9Qkd09blyT/NlfB86zxB9OWH55W01OnH7sfHNfP9Vrp9a6+pqp2T/KaJNe21r7fr+MZ/dtgf9Cv439Lr/d1WzarIcn3Z5zf8VX15f6t0Pclees8jzt17O/PWPf9JCunPZ/rvdlmza216SF/+nF/Mb1Q//2q+ruqemF//R8luSnJX1fV96rq3PmdBgDDTHAFYKnN7P36rSTPTHJ8a23f/PjW1Llu/10Ityc5oKr2mraus5X2O1Pj7dOP3X/NA+dq3Fr7dnoB7eRsfptw0rvl+IYkR/Tr+L0dqSG9252n+4v0epw7rbX9kvznacfdVm/lbendQj1dN8mt86hrW8ftzPh86qbjttaubq2dmt5txJek15Ob1toDrbXfaq09Lcmrkryrql62k7UAMGCCKwCDtk96nxm9t/95yfMW+wX7PZhrk5xfVbv1e+t+YSu77EyNn0tySlX9VH8gpQ9k279//yLJb6YXkP/bjDruT/JgVT0rydvmWcNnk5xdVc/uB+eZ9e+TXg/0hqp6fnqBecqd6d3a/LQ5jn1ZkmdU1b+rquVV9bokz07vtt6d8Y30emffU1W7VtVJ6V2ji/rX7PVVtV9r7bH03pMnkqSqTqmqp/c/y3xfep8L3tqt2QCMAMEVgEH7UJI9k9yV5OtJ/ucSve7r0xvgaH2SP0hycXrzzc5mh2tsrV2f5NfSC6O3J7knvcGDtmbqM6ZXtNbumrb+3emFygeSfKxf83xq+GL/HK5I7zbaK2Y0+dUkH6iqB5K8P/3ey/6+D6f3md6/74/U+4IZx16f5JT0eqXXJ3lPklNm1L3dWmuPphdUT07vff9okrNaazf0m7whyc39W6bfmt71THqDT30pyYNJ/iHJR1trX96ZWgAYvDJeAQAkVXVxkhtaa4ve4wsAbB89rgBMpKo6rqr+l6rapT9dzKnpfVYSABgyywddAAAMyFOS/D/pDZS0LsnbWmv/NNiSAIDZuFUYAACAoeZWYQAAAIaa4AoAAMBQG6nPuB500EHtsMMOG3QZAAAALIJrrrnmrtbaipnrRyq4HnbYYVm7du2gywAAAGARVNX3Z1vvVmEAAACGmuAKAADAUBNcAQAAGGoj9RlXAACA2Tz22GNZt25dNmzYMOhSmIc99tgjq1atyq677jqv9oIrAAAw8tatW5d99tknhx12WKpq0OWwFa21rF+/PuvWrcvhhx8+r33mdatwVb2iqm6sqpuq6txZtu9eVRf3t3+jqg7rr//Zqrqmqr7Vf3zptH2u7B/zuv7XwfOqGAAAYIYNGzbkwAMPFFpHQFXlwAMP3K7e8W32uFbVsiQfSfKzSdYlubqqLm2tfXtas19Ock9r7elVdUaSP0zyuiR3JfmF1tptVfXcJJcnWTltv9e31sxvAwAA7DShdXRs77WaT4/r85Pc1Fr7Xmvt0SQXJTl1RptTk3yqv/y5JC+rqmqt/VNr7bb++uuT7FlVu29XhQAAAENu/fr1Wb16dVavXp2nPOUpWbly5abnjz766Fb3Xbt2bX7jN35jm6/xohe9aEFqvfLKK3PKKacsyLGWynw+47oyyb9Ne74uyfFztWmtbayq+5IcmF6P65RfTHJta+2Raes+UVWPJ/nLJH/QWmszX7yq3pzkzUnS7XbnUS4AAMDSOvDAA3PdddclSc4///zsvffeefe7371p+8aNG7N8+ezxa82aNVmzZs02X+NrX/vawhQ7gpZkOpyqek56tw+/Zdrq17fWjkzy4v7XG2bbt7V2YWttTWttzYoVKxa/WAAAgAVw9tln561vfWuOP/74vOc978k//uM/5oUvfGGOOeaYvOhFL8qNN96YZPMe0PPPPz/nnHNOTjrppDztaU/Lhz/84U3H23vvvTe1P+mkk3L66afnWc96Vl7/+tdnqg/wsssuy7Oe9awce+yx+Y3f+I1t9qzefffdOe2003LUUUflBS94Qb75zW8mSf7u7/5uU4/xMccckwceeCC33357TjzxxKxevTrPfe5z85WvfGXB37O5zKfH9dYknWnPV/XXzdZmXVUtT7JfkvVJUlWrknw+yVmtte9O7dBau7X/+EBV/UV6tyT/+Q6eBwAAQM873pH0ez8XzOrVyYc+tN27rVu3Ll/72teybNmy3H///fnKV76S5cuX50tf+lJ+7/d+L3/5l3+5xT433HBDvvzlL+eBBx7IM5/5zLztbW/bYtqYf/qnf8r111+fpz71qTnhhBPy93//91mzZk3e8pa35Kqrrsrhhx+eM888c5v1nXfeeTnmmGNyySWX5IorrshZZ52V6667LhdccEE+8pGP5IQTTsiDDz6YPfbYIxdeeGFe/vKX573vfW8ef/zxPPzww9v9fuyo+QTXq5McUVWHpxdQz0jy72a0uTTJG5P8Q5LTk1zRWmtVtX+SLyQ5t7X291ON++F2/9baXVW1a5JTknxpp88GAABgiLz2ta/NsmXLkiT33Xdf3vjGN+Zf/uVfUlV57LHHZt3nla98ZXbffffsvvvuOfjgg3PHHXdk1apVm7V5/vOfv2nd6tWrc/PNN2fvvffO0572tE1TzJx55pm58MILt1rfV7/61U3h+aUvfWnWr1+f+++/PyeccELe9a535fWvf31e85rXZNWqVTnuuONyzjnn5LHHHstpp52W1atX79R7sz22GVz7n1l9e3ojAi9L8metteur6gNJ1rbWLk3y8SSfrqqbktydXrhNkrcneXqS91fV+/vrfi7JQ0ku74fWZemF1o8t4HkBAACTagd6RhfLk570pE3L73vf+/KSl7wkn//853PzzTfnpJNOmnWf3Xf/8Xi2y5Yty8aNG3eozc4499xz88pXvjKXXXZZTjjhhFx++eU58cQTc9VVV+ULX/hCzj777LzrXe/KWWedtaCvO5f59LimtXZZkstmrHv/tOUNSV47y35/kOQP5jjssfMvEwAAYLTdd999WbmyNzvoJz/5yQU//jOf+cx873vfy80335zDDjssF1988Tb3efGLX5zPfOYzed/73pcrr7wyBx10UPbdd99897vfzZFHHpkjjzwyV199dW644YbsueeeWbVqVd70pjflkUceybXXXrtkwXVJBmcCAACYdO95z3vyu7/7uznmmGMWvIc0Sfbcc8989KMfzSte8Yoce+yx2WeffbLffvttdZ/zzz8/11xzTY466qice+65+dSnerOcfuhDH8pzn/vcHHXUUdl1111z8skn58orr8zRRx+dY445JhdffHF+8zd/c8HPYS41yww0Q2vNmjVt7dq1gy4DAAAYMt/5znfykz/5k4MuY+AefPDB7L333mmt5dd+7ddyxBFH5J3vfOegy5rVbNesqq5prW0xN5Ae14XyxBPJHXcMugoAAGCCfexjH8vq1avznOc8J/fdd1/e8pa3bHunETCvz7gyD3/wB8n55ycbNiS77TboagAAgAn0zne+c2h7WHeGHteFsmpV0lpy68wpbgEAANgZgutC6XR6j//2b4OtAwAAYMwIrgul2+09Cq4AAAALSnBdKFM9rrfcMtg6AAAAxozgulD22is54AA9rgAAMIFe8pKX5PLLL99s3Yc+9KG87W1vm3Ofk046KVPTff78z/987r333i3anH/++bngggu2+tqXXHJJvv3tb296/v73vz9f+tKXtqf8WV155ZU55ZRTdvo4C0FwXUidjuAKAAAT6Mwzz8xFF1202bqLLrooZ5555rz2v+yyy7L//vvv0GvPDK4f+MAH8jM/8zM7dKxhJbguJMEVAAAm0umnn54vfOELefTRR5MkN998c2677ba8+MUvztve9rasWbMmz3nOc3LeeefNuv9hhx2Wu+66K0nywQ9+MM94xjPyUz/1U7nxxhs3tfnYxz6W4447LkcffXR+8Rd/MQ8//HC+9rWv5dJLL81v//ZvZ/Xq1fnud7+bs88+O5/73OeSJH/7t3+bY445JkceeWTOOeecPPLII5te77zzzsvznve8HHnkkbnhhhu2en533313TjvttBx11FF5wQtekG9+85tJkr/7u7/L6tWrs3r16hxzzDF54IEHcvvtt+fEE0/M6tWr89znPjdf+cpXdu7NjXlcF1a3m3zta4OuAgAAJto73pFcd93CHnP16uRDH5p7+wEHHJDnP//5+eIXv5hTTz01F110UX7pl34pVZUPfvCDOeCAA/L444/nZS97Wb75zW/mqKOOmvU411xzTS666KJcd9112bhxY573vOfl2GOPTZK85jWvyZve9KYkyb//9/8+H//4x/Prv/7redWrXpVTTjklp59++mbH2rBhQ84+++z87d/+bZ7xjGfkrLPOyp/8yZ/kHe94R5LkoIMOyrXXXpuPfvSjueCCC/Knf/qnc57feeedl2OOOSaXXHJJrrjiipx11lm57rrrcsEFF+QjH/lITjjhhDz44IPZY489cuGFF+blL3953vve9+bxxx/Pww8/vD1v9az0uC6kTie5++7koYcGXQkAALDEpt8uPP024c9+9rN53vOel2OOOSbXX3/9Zrf1zvSVr3wlr371q7PXXntl3333zate9apN2/75n/85L37xi3PkkUfmM5/5TK6//vqt1nPjjTfm8MMPzzOe8YwkyRvf+MZcddVVm7a/5jWvSZIce+yxufnmm7d6rK9+9at5wxvekCR56UtfmvXr1+f+++/PCSeckHe961358Ic/nHvvvTfLly/Pcccdl0984hM5//zz861vfSv77LPPVo89H3pcF9L0uVyf9azB1gIAABNqaz2ji+nUU0/NO9/5zlx77bV5+OGHc+yxx+Zf//Vfc8EFF+Tqq6/Ok5/85Jx99tnZsGHDDh3/7LPPziWXXJKjjz46n/zkJ3PllVfuVL277757kmTZsmXZuHHjDh3j3HPPzStf+cpcdtllOeGEE3L55ZfnxBNPzFVXXZUvfOELOfvss/Oud70rZ5111k7Vqsd1IU0PrgAAwETZe++985KXvCTnnHPOpt7W+++/P0960pOy33775Y477sgXv/jFrR7jxBNPzCWXXJIf/ehHeeCBB/JXf/VXm7Y98MADOfTQQ/PYY4/lM5/5zKb1++yzTx544IEtjvXMZz4zN998c2666aYkyac//en89E//9A6d24tf/OJNr3nllVfmoIMOyr777pvvfve7OfLII/M7v/M7Oe6443LDDTfk+9//fg455JC86U1vyq/8yq/k2muv3aHXnE6P60ISXAEAYKKdeeaZefWrX73pluGjjz46xxxzTJ71rGel0+nkhBNO2Or+z3ve8/K6170uRx99dA4++OAcd9xxm7b9/u//fo4//visWLEixx9//KawesYZZ+RNb3pTPvzhD28alClJ9thjj3ziE5/Ia1/72mzcuDHHHXdc3vrWt+7QeZ1//vk555xzctRRR2WvvfbKpz71qSS9KX++/OUvZ5dddslznvOcnHzyybnooovyR3/0R9l1112z995758///M936DWnq9baTh9kqaxZs6ZNzXM0lB59NNljj+S883pfAADAkvjOd76Tn/zJnxx0GWyH2a5ZVV3TWlszs61bhRfSbrslT3lKcsstg64EAABgbAiuC81crgAAAAtKcF1ogisAAMCCElwX2lRwHaHPDgMAwDgYpfF7Jt32XivBdaF1u8lDDyX33jvoSgAAYGLsscceWb9+vfA6AlprWb9+ffbYY49572M6nIU2NSXOLbckT37yYGsBAIAJsWrVqqxbty533nnnoEthHvbYY4+sWrVq3u0F14U2fS7Xo48ebC0AADAhdt111xx++OGDLoNF4lbhhTY9uAIAALDTBNeFdsghyfLlgisAAMACEVwX2rJlyapVgisAAMACEVwXQ6fTG5wJAACAnSa4LoapuVwBAADYaYLrYuh0knXrkieeGHQlAAAAI09wXQydTvLYY8kPfzjoSgAAAEae4LoYut3eo9uFAQAAdprguhim5nI1QBMAAMBOE1wXw1Rw1eMKAACw0wTXxXDAAcmeewquAAAAC0BwXQxVpsQBAABYIILrYul2BVcAAIAFILgulk7H4EwAAAALQHBdLJ1OcvvtvflcAQAA2GGC62LpdJLWkttuG3QlAAAAI01wXSymxAEAAFgQguti6XZ7j4IrAADAThFcF8tUj6sBmgAAAHaK4LpY9t472X9/Pa4AAAA7SXBdTJ2O4AoAALCTBNfFJLgCAADsNMF1MXW7gisAAMBOElwXU6eT3HVX8vDDg64EAABgZAmui2lqZOF16wZbBwAAwAgTXBfTVHB1uzAAAMAOE1wXk+AKAACw0wTXxbRqVe9RcAUAANhhguti2n335JBDkltuGXQlAAAAI0twXWzmcgUAANgpgutiE1wBAAB2iuC62ARXAACAnSK4LrZuN3nggeS++wZdCQAAwEgSXBfb1JQ4BmgCAADYIYLrYjOXKwAAwE4RXBeb4AoAALBTBNfFduihybJlgisAAMAOElwX27JlycqVgisAAMAOElyXQqdjcCYAAIAdJLguBXO5AgAA7DDBdSl0Osm6dUlrg64EAABg5AiuS6HTSR55JLnzzkFXAgAAMHIE16XQ7fYe3S4MAACw3QTXpTA1l6sBmgAAALab4LoUpoKrHlcAAIDtJrguhYMOSvbYQ3AFAADYAYLrUqhKVq0SXAEAAHaA4LpUul3BFQAAYAfMK7hW1Suq6saquqmqzp1l++5VdXF/+zeq6rD++p+tqmuq6lv9x5dO2+fY/vqbqurDVVULdVJDqdMxOBMAAMAO2GZwraplST6S5OQkz05yZlU9e0azX05yT2vt6Un+U5I/7K+/K8kvtNaOTPLGJJ+ets+fJHlTkiP6X6/YifMYfp1OctttycaNg64EAABgpMynx/X5SW5qrX2vtfZokouSnDqjzalJPtVf/lySl1VVtdb+qbV2W3/99Un27PfOHppk39ba11trLcmfJzltp89mmHU6yRNPJLffPuhKAAAARsp8guvKJNM/nLmuv27WNq21jUnuS3LgjDa/mOTa1toj/fbrtnHMJElVvbmq1lbV2jvvvHMe5Q4pU+IAAADskCUZnKmqnpPe7cNv2d59W2sXttbWtNbWrFixYuGLWyrdbu9RcAUAANgu8wmutybpTHu+qr9u1jZVtTzJfknW95+vSvL5JGe11r47rf2qbRxzvEz1uBqgCQAAYLvMJ7heneSIqjq8qnZLckaSS2e0uTS9wZeS5PQkV7TWWlXtn+QLSc5trf39VOPW2u1J7q+qF/RHEz4ryX/fyXMZbvvu2/vS4woAALBdthlc+59ZfXuSy5N8J8lnW2vXV9UHqupV/WYfT3JgVd2U5F1JpqbMeXuSpyd5f1Vd1/86uL/tV5P8aZKbknw3yRcX6qSGVqcjuAIAAGyn6g3qOxrWrFnT1q5dO+gydtzJJyd33pmM8jkAAAAskqq6prW2Zub6JRmcib5u12dcAQAAtpPgupQ6nV6P64YNg64EAABgZAiuS2lqZOF167beDgAAgE0E16U0FVwN0AQAADBvgutSElwBAAC2m+C6lFat6j0aoAkAAGDeBNeltOeeyYoVelwBAAC2g+C61DodwRUAAGA7CK5LTXAFAADYLoLrUhNcAQAAtovgutS63eS++5L77x90JQAAACNBcF1qpsQBAADYLoLrUhNcAQAAtovgutQEVwAAgO0iuC61pz412WUXwRUAAGCeBNeltnx5L7zecsugKwEAABgJgusgmBIHAABg3gTXQRBcAQAA5k1wHYSp4NraoCsBAAAYeoLrIHQ6yYYNyfr1g64EAABg6Amug9Dt9h4N0AQAALBNgusgmMsVAABg3gTXQRBcAQAA5k1wHYQVK5LddhNcAQAA5kFwHYRddklWrRJcAQAA5kFwHZRu1+BMAAAA8yC4DsrUXK4AAABsleA6KJ1OcuutyeOPD7oSAACAoSa4Dkqn0wutP/jBoCsBAAAYaoLroJgSBwAAYF4E10HpdnuPBmgCAADYKsF1UPS4AgAAzIvgOij77ZfsvbfgCgAAsA2C66BUmRIHAABgHgTXQRJcAQAAtklwHaRu1+BMAAAA2yC4DlKnk9xxR/LII4OuBAAAYGgJroM0NbLwrbcOtg4AAIAhJrgOkilxAAAAtklwHSTBFQAAYJsE10GaCq4GaAIAAJiT4DpIe+2VHHigHlcAAICtEFwHzVyuAAAAWyW4DprgCgAAsFWC66AJrgAAAFsluA5at5vcc0/y4IODrgQAAGAoCa6DZkocAACArRJcB01wBQAA2CrBddAEVwAAgK0SXAdt5cqkSnAFAACYg+A6aLvumhx6aHLLLYOuBAAAYCgJrsPAlDgAAABzElyHgeAKAAAwJ8F1GEwF19YGXQkAAMDQEVyHQaeTPPxwcs89g64EAABg6Aiuw6Db7T0aoAkAAGALguswMJcrAADAnATXYSC4AgAAzElwHQaHHNKbz1VwBQAA2ILgOgx22SVZuVJwBQAAmIXgOiy6XYMzAQAAzEJwHRZTc7kCAACwGcF1WHQ6ya23Jk88MehKAAAAhorgOiw6neSxx5I77hh0JQAAAENFcB0WpsQBAACYleA6LLrd3qMBmgAAADYjuA4LPa4AAACzElyHxZOfnOy1l+AKAAAwg+A6LKpMiQMAADALwXWYdLuCKwAAwAyC6zDpdAzOBAAAMIPgOkw6neQHP0gefXTQlQAAAAyNeQXXqnpFVd1YVTdV1bmzbN+9qi7ub/9GVR3WX39gVX25qh6sqj+esc+V/WNe1/86eCFOaKR1OklryW23DboSAACAobHN4FpVy5J8JMnJSZ6d5MyqevaMZr+c5J7W2tOT/Kckf9hfvyHJ+5K8e47Dv761trr/9cMdOYGxYkocAACALcynx/X5SW5qrX2vtfZokouSnDqjzalJPtVf/lySl1VVtdYeaq19Nb0Ay7Z0u71HwRUAAGCT+QTXlUmmJ6l1/XWztmmtbUxyX5ID53HsT/RvE35fVdVsDarqzVW1tqrW3nnnnfM45Aib6nE1QBMAAMAmgxyc6fWttSOTvLj/9YbZGrXWLmytrWmtrVmxYsWSFrjknvSk5MlP1uMKAAAwzXyC661JOtOer+qvm7VNVS1Psl+S9Vs7aGvt1v7jA0n+Ir1bkul0BFcAAIBp5hNcr05yRFUdXlW7JTkjyaUz2lya5I395dOTXNFaa3MdsKqWV9VB/eVdk5yS5J+3t/ixJLgCAABsZvm2GrTWNlbV25NcnmRZkj9rrV1fVR9Isra1dmmSjyf5dFXdlOTu9MJtkqSqbk6yb5Ldquq0JD+X5PtJLu+H1mVJvpTkYwt6ZqOq202+/vVBVwEAADA0thlck6S1dlmSy2ase/+05Q1JXjvHvofNcdhj51fihOl0kvXrk4cfTvbaa9DVAAAADNwgB2diNuZyBQAA2IzgOmwEVwAAgM0IrsNGcAUAANiM4DpsVq5MqpJbbhl0JQAAAENBcB02u++eHHKIHlcAAJ0ZIRcAABrHSURBVIA+wXUYmcsVAABgE8F1GAmuAAAAmwiuw2gquLY26EoAAAAGTnAdRt1u8uCDyb33DroSAACAgRNch5EpcQAAADYRXIeR4AoAALCJ4DqMBFcAAIBNBNdh9JSnJMuXC64AAAARXIfTsmXJypXJLbcMuhIAAICBE1yHlblcAQAAkgiuw0twBQAASCK4Dq9OJ1m3LnniiUFXAgAAMFCC67DqdJJHH03uvHPQlQAAAAyU4Dqsut3eowGaAACACSe4DitzuQIAACQRXIeX4AoAAJBEcB1eBx6Y7LGH4AoAAEw8wXVYVZkSBwAAIILrcOt2Dc4EAABMPMF1mOlxBQAAEFyHWqeT3H57snHjoCsBAAAYGMF1mHU6yRNPJLfdNuhKAAAABkZwHWamxAEAABBch1q323s0QBMAADDBBNdhpscVAABAcB1q++yT7Lef4AoAAEw0wXXYmRIHAACYcILrsBNcAQCACSe4Drtu1+BMAADARBNch12nk9x1V/KjHw26EgAAgIEQXIfd1MjC69YNtg4AAIABEVyHnSlxAACACSe4DjvBFQAAmHCC67Bbtar3aIAmAABgQgmuw26PPZKDD9bjCgAATCzBdRSYyxUAAJhggusoEFwBAIAJJriOAsEVAACYYILrKOh2k/vvT+67b9CVAAAALDnBdRSYEgcAAJhggusoEFwBAIAJJriOAsEVAACYYILrKDj00GSXXQRXAABgIgmuo2D58mTlyuSWWwZdCQAAwJITXEeFKXEAAIAJJbiOCsEVAACYUILrqOh0knXrktYGXQkAAMCSElxHRaeTbNiQ3HXXoCsBAABYUoLrqOh2e48GaAIAACaM4DoqzOUKAABMKMF1VAiuAADAhBJcR8WKFcnuuwuuAADAxBFcR0VVsmqV4AoAAEwcwXWUdLsGZwIAACaO4DpKOh09rgAAwMQRXEdJp5Pcdlvy+OODrgQAAGDJCK6jpNPphdbbbx90JQAAAEtGcB0lpsQBAAAmkOA6Srrd3qMBmgAAgAkiuI4SPa4AAMAEElxHyX77JfvsI7gCAAATRXAdNabEAQAAJozgOmoEVwAAYMIIrqOm2zU4EwAAMFEE11HT6SQ//GHyyCODrgQAAGBJCK6jZmpk4XXrBlsHAADAEhFcR40pcQAAgAkzr+BaVa+oqhur6qaqOneW7btX1cX97d+oqsP66w+sqi9X1YNV9ccz9jm2qr7V3+fDVVULcUJjT3AFAAAmzDaDa1UtS/KRJCcneXaSM6vq2TOa/XKSe1prT0/yn5L8YX/9hiTvS/LuWQ79J0nelOSI/tcrduQEJs5UcDVAEwAAMCHm0+P6/CQ3tda+11p7NMlFSU6d0ebUJJ/qL38uycuqqlprD7XWvppegN2kqg5Nsm9r7euttZbkz5OctjMnMjH23DM56CA9rgAAwMSYT3BdmWR6SlrXXzdrm9baxiT3JTlwG8ecPrrQbMdMklTVm6tqbVWtvfPOO+dR7gQwlysAADBBhn5wptbaha21Na21NStWrBh0OcNBcAUAACbIfILrrUk6056v6q+btU1VLU+yX5L12zjmqm0ck7kIrgAAwASZT3C9OskRVXV4Ve2W5Iwkl85oc2mSN/aXT09yRf+zq7Nqrd2e5P6qekF/NOGzkvz37a5+UnW7yb33Jg88MOhKAAAAFt3ybTVorW2sqrcnuTzJsiR/1lq7vqo+kGRta+3SJB9P8umquinJ3emF2yRJVd2cZN8ku1XVaUl+rrX27SS/muSTSfZM8sX+F/MxfUqcZ88c4BkAAGC8bDO4Jklr7bIkl81Y9/5pyxuSvHaOfQ+bY/3aJM+db6FMI7gCAAATZOgHZ2IW04MrAADAmBNcR9FTn5pUCa4AAMBEEFxH0a679sLrLbcMuhIAAIBFJ7iOKlPiAAAAE0JwHVWCKwAAMCEE11E1FVznni4XAABgLAiuo6rTSX70o2T9+kFXAgAAsKgE11HV7fYe3S4MAACMOcF1VJnLFQAAmBCC66gSXAEAgAkhuI6qgw/uzecquAIAAGNOcB1Vu+ySrFqV3HLLoCsBAABYVILrKOt29bgCAABjT3AdZVNzuQIAAIwxwXWUdTrJrbcmjz8+6EoAAAAWjeA6yjqdZOPG5I47Bl0JAADAohFcR9nUlDgGaAIAAMaY4DrKut3eo8+5AgAAY0xwHWVTPa6CKwAAMMYE11G2//7Jk54kuAIAAGNNcB1lVabEAQAAxp7gOuo6HYMzAQAAY01wHXXdrh5XAABgrAmuo67T6c3j+uijg64EAABgUQiuo67TSVpLbr110JUAAAAsCsF11JkSBwAAGHOC66ibCq4GaAIAAMaU4Drq9LgCAABjTnAddU96UnLAAYIrAAAwtgTXcdDpCK4AAMDYElzHgeAKAACMMcF1HHQ6BmcCAADGluA6Drrd5J57koceGnQlAAAAC05wHQdGFgYAAMaY4DoOBFcAAGCMCa7jQHAFAADGmOA6DlauTKoM0AQAAIwlwXUc7LZb8pSn6HEFAADGkuA6LszlCgAAjCnBdVwIrgAAwJgSXMfFVHBtbdCVAAAALCjBdVx0OslDDyX33DPoSgAAABaU4Douut3eo9uFAQCAMSO4jgtzuQIAAGNKcB0XgisAADCmBNdxccghyfLlgisAADB2BNdxsWxZsnJlcsstg64EAABgQQmu46Tb1eMKAACMHcF1nEzN5QoAADBGBNdx0ukk69YlTzwx6EoAAAAWjOA6Tjqd5LHHkh/+cNCVAAAALBjBdZx0u71HAzQBAABjRHAdJ+ZyBQAAxpDgOk4EVwAAYAwJruPkgAOSPfcUXAEAgLEiuI6TKlPiAAAAY0dwHTfdrsGZAACAsSK4jhs9rgAAwJgRXMdNp5PcfntvPlcAAIAxILiOm04naS257bZBVwIAALAgBNdxY0ocAABgzAiu46bb7T0aoAkAABgTguu40eMKAACMGcF13Oy9d7L//oIrAAAwNgTXcWRKHAAAYIwIruNIcAUAAMaI4DqOul2DMwEAAGNDcB1HnU6yfn3y8MODrgQAAGCnCa7jaGpk4XXrBlsHAADAAhBcx5EpcQAAgDEiuI4jwRUAABgjgus4WrWq92iAJgAAYAzMK7hW1Suq6saquqmqzp1l++5VdXF/+zeq6rBp2363v/7Gqnr5tPU3V9W3quq6qlq7ECdD3+67J4ccoscVAAAYC8u31aCqliX5SJKfTbIuydVVdWlr7dvTmv1ykntaa0+vqjOS/GGS11XVs5OckeQ5SZ6a5EtV9YzW2uP9/V7SWrtrAc+HKeZyBQAAxsR8elyfn+Sm1tr3WmuPJrkoyakz2pya5FP95c8leVlVVX/9Ra21R1pr/5rkpv7xWGyCKwAAMCbmE1xXJpmegNb1183aprW2Mcl9SQ7cxr4tyV9X1TVV9ebtL52t6nR6n3FtbdCVAAAA7JRt3iq8iH6qtXZrVR2c5G+q6obW2lUzG/VD7ZuTpNvtLnWNo6vbTR58MLnvvmT//QddDQAAwA6bT4/rrUk6056v6q+btU1VLU+yX5L1W9u3tTb1+MMkn88ctxC31i5sra1pra1ZsWLFPMoliSlxAACAsTGf4Hp1kiOq6vCq2i29wZYundHm0iRv7C+fnuSK1lrrrz+jP+rw4UmOSPKPVfWkqtonSarqSUl+Lsk/7/zpsIngCgAAjIlt3ircWttYVW9PcnmSZUn+rLV2fVV9IMna1tqlST6e5NNVdVOSu9MLt+m3+2ySbyfZmOTXWmuPV9UhST7fG78py5P8RWvtfy7C+U0uwRUAABgT8/qMa2vtsiSXzVj3/mnLG5K8do59P5jkgzPWfS/J0dtbLNvh0EOTZcsEVwAAYOTN51ZhRtGyZcnKlb2RhQEAAEaY4DrOzOUKAACMAcF1nAmuAADAGBBcx1mnk6xbl7Q26EoAAAB2mOA6zjqd5JFHkjvvHHQlAAAAO0xwHWfdbu/RAE0AAMAIE1zHmblcAQCAMSC4jjPBFQAAGAOC6zg76KBkjz0EVwAAYKQJruOsKlm1ymdcAQCAkSa4jrtuV48rAAAw0gTXcdfpCK4AAMBIE1zHXaeT3HZbsnHjoCsBAADYIYLruOt0kieeSG6/fdCVAAAA7BDBddxNTYljgCYAAGBECa7jrtvtPfqcKwAAMKIE13E31eMquAIAACNKcB13++7b+xJcAQCAESW4TgJT4gAAACNMcJ0EnY7BmQAAgJEluE6CblePKwAAMLIE10nQ6SR33pls2DDoSgAAALab4DoJpkYWXrdusHUAAADsAMF1EpgSBwAAGGGC6ySYCq4GaAIAAEaQ4DoJVq3qPepxBQAARpDgOgn23DNZsUJwBQAARpLgOik6HcEVAAAYSYLrpBBcAQCAESW4TopOx+BMAADASBJcJ0W3m9x/f+8LAABghAiuk8JcrgAAwIgSXCeF4AoAAIwowXVSCK4AAMCIElwnxVOfmuyyiwGaAACAkSO4Torly3vhVY8rAAAwYgTXSWIuVwAAYAQJrpNEcAUAAEaQ4DpJpoJra4OuBAAAYN4E10nS6SQbNiR33TXoSgAAAOZNcJ0k3W7v0e3CAADACBFcJ4m5XAEAgBEkuE4SwRUAABhBguskWbEi2W03wRUAABgpgusk2WWXZNWq5JZbBl0JAADAvAmuk6bb1eMKAACMFMF10kzN5QoAADAiBNdJ0+kkt96aPP74oCsBAACYF8F10nQ6vdD6gx8MuhIAAIB5EVwnzdSUOAZoAgAARoTgOmm63d6jz7kCAAAjQnCdNFM9roIrAAAwIgTXSbPffsneewuuAADAyBBcJ02VKXEAAICRIrhOok7H4EwAAMDIEFwnUberxxUAABgZgusk6nSSO+5IHnlk0JUAAABsk+A6iaZGFr711sHWAQAAMA+C6yQyJQ4AADBCBNdJNBVcDdAEAACMAMF1EulxBQAARojgOon22is58EDBFQAAGAmC66TqdARXAABgJAiuk0pwBQAARoTgOqk6HYMzAQAAI0FwnVTdbnLvvcmDDw66EgAAgK0SXCeVkYUBAIARIbhOKsEVAAAYEYLrpBJcAQCAESG4TqqVK5MqAzQBAABDT3CdVLvumhx6qB5XAABg6Amuk8xcrgAAwAiYV3CtqldU1Y1VdVNVnTvL9t2r6uL+9m9U1WHTtv1uf/2NVfXy+R6TJSC4AgAAI2D5thpU1bIkH0nys0nWJbm6qi5trX17WrNfTnJPa+3pVXVGkj9M8rqqenaSM5I8J8lTk3ypqp7R32dbx2SxdTrJZZclrfU+7wrA2Gut9/XEE4v31dqPX2v6a862vBjbd/SYU7XPXB7mbe2JtsX29kSb0a711295rKltTzw+c/301/rx/i3zbDtLvT/+R7jFv8okyZZ/iWzesOZYP7Wq96dM22J9klTNvn7Tsae2z1Jb1Y+PX9U2X+7XNdfy1LE3W55+vKlzn9Fms2O1ee43y7rpJ7XF+9d+vM9m57vZ0x8fc+b78+N9px2vMuNiz/Iarc1yvC2vT5vxfK7mbeYppKW12ux5r11t9nzWfXfwdTe9E/3X2OLnzYztW65rW+47vYw5tm1aP1tNc2x74ZqN+a2PPSujaJvBNcnzk9zUWvteklTVRUlOTTI9ZJ6a5Pz+8ueS/HFVVX/9Ra21R5L8a1Xd1D9e5nHMkXLxxcnHP744x97yG3KB3PKbycMnJ0++OrP9uphWwcK83rwOM7/X2vIH7Q5YsPd1YS/Q5j9sF/bVt+fYi/XPbtC29m9nW/+udm7fufR/oaZ6X9N+0Uz9+dBabdo+fdvMtjPbz71t2rG3eN3pf4rMvW36eW/6YylJ5YnN/ojafHv/+dQfYrMs17Q/kqrNsu+M51t9nS3ati3Oe9blaec713uyrf2mr998Xdv6MWbu1ypPZJf+1/TlGV9tHm36X82nhEbaLnl8s3/jvavetviabf32tF2sY89lZtRayO0Lse98v2/n/t5fiv02/12xrXMctnWz/fsYpXXTf+fM53Fh2rbt3qf70M1Jxje4rkwy/X7SdUmOn6tNa21jVd2X5MD++q/P2Hdlf3lbxxwpjzySPPjg4h1/UTpEn3Rw8uSHkicWKKJsq8YFOodt/QJa8BdcgMP0fiBvx0vO/J/hbbVfpGOPZD/8VoreWnDf1ruy1T9stvEfAlv/o2jm/8q3Td/v0//8mPof9M3+Jz5ti/+Zn/V/+PPErP9DP69953jd1MyA3P8V2mrattnCdzZ7vnmsnCNcz7nvj4P5EzND/GxBvNU2znPm+1HTznvr78+2lqcfd8vltuX7O237sl1aL27WfL6eyC6VWdZNLWce67e1betfU/+Gf3wemXaesy9PtZt9ebb3ZtvH3/oxZ+zTWnbZ5cf/InepH59D7VKbndcu076fttg2/fxnPN+0vEtt8Rq71I/Xb7HfLpuf22Y2nciMx61tG9Y2O/u4FK8x/XGh2iz08RbrXJZy3ynb83xY9u2t3Pxx1jaz7TaPdgvRZrfO/OoZQvMJrgNVVW9O8uYk6Xa7A65mbmed1fsaLXsmefagiwAAANiq+dwvdGuS6dF8VX/drG2qanmS/ZKs38q+8zlmkqS1dmFrbU1rbc2KFSvmUS4AAADjZD7B9eokR1TV4VW1W3qDLV06o82lSd7YXz49yRWttdZff0Z/1OHDkxyR5B/neUwAAADY9q3C/c+svj3J5UmWJfmz1tr1VfWBJGtba5cm+XiST/cHX7o7vSCafrvPpjfo0sYkv9ZaezxJZjvmwp8eAAAAo67aog1Zu/DWrFnT1q5dO+gyAAAAWARVdU1rbc3M9cbEBwAAYKgJrgAAAAw1wRUAAIChJrgCAAAw1ARXAAAAhprgCgAAwFATXAEAABhqgisAAABDTXAFAABgqAmuAAAADDXBFQAAgKEmuAIAADDUBFcAAACGmuAKAADAUKvW2qBrmLequjPJ9wddx1YclOSuQRfBTnENR59rONpcv9HnGo4+13D0uYajbdKv30+01lbMXDlSwXXYVdXa1tqaQdfBjnMNR59rONpcv9HnGo4+13D0uYajzfWbnVuFAQAAGGqCKwAAAENNcF1YFw66AHaaazj6XMPR5vqNPtdw9LmGo881HG2u3yx8xhUAAIChpscVAACAoSa47oCqekVV3VhVN1XVubNs372qLu5v/0ZVHbb0VTKXqupU1Zer6ttVdX1V/eYsbU6qqvuq6rr+1/sHUStzq6qbq+pb/euzdpbtVVUf7n8ffrOqnjeIOtlSVT1z2vfWdVV1f1W9Y0Yb34NDpqr+rKp+WFX/PG3dAVX1N1X1L/3HJ8+x7xv7bf6lqt64dFUz3RzX8I+q6ob+z8nPV9X+c+y71Z+5LI05ruH5VXXrtJ+XPz/Hvlv9+5XFN8f1u3jatbu5qq6bY9+J/x50q/B2qqplSf6/JD+bZF2Sq5Oc2Vr79rQ2v5rkqNbaW6vqjCSvbq29biAFs4WqOjTJoa21a6tqnyTXJDltxjU8Kcm7W2unDKhMtqGqbk6yprU26zxn/V/cv57k55Mcn+T/bK0dv3QVMh/9n6m3Jjm+tfb9aetPiu/BoVJVJyZ5MMmft9ae21/3H5Pc3Vr7D/0/hJ/cWvudGfsdkGRtkjVJWno/c49trd2zpCfAXNfw55Jc0VrbWFV/mCQzr2G/3c3Zys9clsYc1/D8JA+21i7Yyn7b/PuVxTfb9Zux/X9Pcl9r7QOzbLs5E/49qMd1+z0/yU2tte+11h5NclGSU2e0OTXJp/rLn0vysqqqJayRrWit3d5au7a//ECS7yRZOdiqWASnpveLobXWvp5k//5/WjBcXpbku9NDK8OptXZVkrtnrJ7+++5TSU6bZdeXJ/mb1trd/bD6N0lesWiFMqfZrmFr7a9baxv7T7+eZNWSF8a8zfF9OB/z+fuVRba169fPCr+U5L8uaVEjRHDdfiuT/Nu05+uyZejZ1Kb/y+C+JAcuSXVsl/5t3Mck+cYsm19YVf9vVX2xqp6zpIUxHy3JX1fVNVX15lm2z+d7lcE7I3P/kvY9OPwOaa3d3l/+QZJDZmnje3F0nJPki3Ns29bPXAbr7f3bvf9sjlv2fR8OvxcnuaO19i9zbJ/470HBlYlVVXsn+csk72it3T9j87VJfqK1dnSS/yvJJUtdH9v0U6215yU5Ocmv9W+/YYRU1W5JXpXkv82y2ffgiGm9zx75/NGIqqr3JtmY5DNzNPEzd3j9Sf7/9u7n1aYoCuD4d+XJwEDqFQklmb+BpEwMeCEpMniSX1EoY8VEMTFhrPyYCDEgbyDyDygyQQwMKNJTDCQmWAbnPN2uc3Q95ezX+34m995z9mDfdmufs/bdZ11YDowA74Az3XZHU7SDP//aOuNj0MT1770FlvR8Xlwfa2wTEUPAPODDf+mdBhIRs6mS1iuZebP/fGZ+yszP9fs7wOyIGP7P3dQfZObb+vU9cItqG1SvQWJV3doIPM7Mif4TxuC0MTG5Bb9+fd/QxlgsXETsBTYDO7Ol+MkAc646kpkTmfk9M38A52keG+OwYHW+sA243tbGGDRxnYqHwIqIWFb/WjAGjPe1GQcmqyZupyp64Cp0IepnCC4CzzPzbEubhZPPJUfEKqpYcfGhEBExty6sRUTMBUaBp33NxoHdUVlNVezgHSpJ6+qyMTht9F7v9gC3G9rcA0YjYn69hXG0PqYCRMQG4CiwJTO/tLQZZM5VR/rqN2yleWwGuX9Vd9YBLzLzTdNJY7Ay1HUHppu66t4RqovuLOBSZj6LiJPAo8wcp0qKLkfES6oHsMe667EarAF2AU96So4fB5YCZOY5qgWHwxHxDfgKjLn4UJQFwK06rxkCrmbm3Yg4BL/G8A5VReGXwBdgX0d9VYP6wrseONhzrHf8jMHCRMQ1YC0wHBFvgBPAaeBGROwHXlMVFiEiVgKHMvNAZn6MiFNUN84AJzNzKsVl9I9axvAYMAe4X8+pD+p/RVgEXMjMTbTMuR18hRmvZQzXRsQI1Vb9V9Tzau8Ytt2/dvAVZrSm8cvMizTUezAGf+ff4UiSJEmSiuZWYUmSJElS0UxcJUmSJElFM3GVJEmSJBXNxFWSJEmSVDQTV0mSJElS0UxcJUmSJElFM3GVJEmSJBXNxFWSJEmSVLSfxGnbQbeQ/XAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(path)"
      ],
      "metadata": {
        "id": "vI0frdCD-wo5"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_X)\n",
        "real = YTest\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f83cbaa-cd41-4bee-d98a-9894beb0ab63",
        "id": "bt6Evbty-wo5"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0124111],\n",
              "       [1.0205848],\n",
              "       [1.0256045],\n",
              "       ...,\n",
              "       [1.0097868],\n",
              "       [1.005673 ],\n",
              "       [1.0024971]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE Debu: {}'.format(numpy.sqrt(numpy.mean((pred[:, 0]-real[:, 0])**2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d3c648-7c95-424b-ae7c-ed8e754e0997",
        "id": "4sJJECGp-wo5"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Debu: 0.006008579670685285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "z8XyCDmL28DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "ddSRfjUj4_xO"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_csv('/content/dataset_co2.csv', delimiter=';')\n",
        "b = pd.read_csv('/content/dataset_debu.csv', delimiter=';')\n",
        "c = pd.read_csv('/content/dataset_suhu_&_kelembapan.csv', delimiter=';')\n",
        "d = pd.read_csv('/content/dataset_tekanan_udara.csv', delimiter=';')\n",
        "\n",
        "df = pd.concat([a,b,c,d], axis=1)\n",
        "df.dropna(inplace=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9sBivkCl3DyM",
        "outputId": "f47f0aab-b4b6-49ce-bebe-599f47e4e113"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CO2       PM10  Tavg  RH_avg  Pressure\n",
              "0    72.076  13.413329  29.6    76.0    1.0286\n",
              "1    64.442   9.098366  29.1    76.0    1.0318\n",
              "2    64.084   6.363176  26.6    87.0    1.0314\n",
              "3    60.842   4.621669  29.1    78.0    1.0262\n",
              "4    61.798   5.254629  29.7    76.0    1.0246\n",
              "..      ...        ...   ...     ...       ...\n",
              "120  91.658   8.068038  29.4    76.0    1.0198\n",
              "121  92.416   9.587104  27.5    85.0    1.0171\n",
              "122  72.840   7.563549  26.3    88.0    1.0161\n",
              "123  71.410   5.585073  28.1    80.0    1.0175\n",
              "124  82.510   3.669890  28.2    83.0    1.0170\n",
              "\n",
              "[125 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b443bbbb-26ec-462d-8ef4-1174cf3e768a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>Tavg</th>\n",
              "      <th>RH_avg</th>\n",
              "      <th>Pressure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72.076</td>\n",
              "      <td>13.413329</td>\n",
              "      <td>29.6</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64.442</td>\n",
              "      <td>9.098366</td>\n",
              "      <td>29.1</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.084</td>\n",
              "      <td>6.363176</td>\n",
              "      <td>26.6</td>\n",
              "      <td>87.0</td>\n",
              "      <td>1.0314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60.842</td>\n",
              "      <td>4.621669</td>\n",
              "      <td>29.1</td>\n",
              "      <td>78.0</td>\n",
              "      <td>1.0262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61.798</td>\n",
              "      <td>5.254629</td>\n",
              "      <td>29.7</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>91.658</td>\n",
              "      <td>8.068038</td>\n",
              "      <td>29.4</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>92.416</td>\n",
              "      <td>9.587104</td>\n",
              "      <td>27.5</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1.0171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>72.840</td>\n",
              "      <td>7.563549</td>\n",
              "      <td>26.3</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1.0161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>71.410</td>\n",
              "      <td>5.585073</td>\n",
              "      <td>28.1</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>82.510</td>\n",
              "      <td>3.669890</td>\n",
              "      <td>28.2</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1.0170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows  5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b443bbbb-26ec-462d-8ef4-1174cf3e768a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b443bbbb-26ec-462d-8ef4-1174cf3e768a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b443bbbb-26ec-462d-8ef4-1174cf3e768a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scalerKmeans = MinMaxScaler()\n",
        "x_scaled = scalerKmeans.fit_transform(df)\n",
        "x_scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDTYuDtK6GUn",
        "outputId": "858cd449-209b-4915-937a-4af11a6cc25e"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29242317, 1.        , 0.7173913 , 0.3       , 0.77707006],\n",
              "       [0.0988437 , 0.65071756, 0.60869565, 0.3       , 0.84501062],\n",
              "       [0.0897657 , 0.42931276, 0.06521739, 0.85      , 0.83651805],\n",
              "       [0.00755655, 0.28834333, 0.60869565, 0.4       , 0.72611465],\n",
              "       [0.03179836, 0.33957945, 0.73913043, 0.3       , 0.69214437],\n",
              "       [0.1519931 , 0.43357685, 0.73913043, 0.25      , 0.68789809],\n",
              "       [0.30636981, 0.3298294 , 0.60869565, 0.3       , 0.73673036],\n",
              "       [0.37115833, 0.28587297, 0.02173913, 0.95      , 0.69639066],\n",
              "       [0.19913277, 0.20236701, 0.82608696, 0.1       , 0.68577495],\n",
              "       [0.18064712, 0.22773951, 0.58695652, 0.3       , 0.88535032],\n",
              "       [0.1642408 , 0.17652936, 0.34782609, 0.6       , 0.96178344],\n",
              "       [0.26813064, 0.20112133, 0.        , 0.9       , 0.84713376],\n",
              "       [0.25372756, 0.16176697, 0.47826087, 0.4       , 0.64968153],\n",
              "       [0.06047774, 0.16326874, 0.76086957, 0.2       , 0.6581741 ],\n",
              "       [0.10079623, 0.23004456, 0.58695652, 0.25      , 0.83014862],\n",
              "       [0.        , 0.29005016, 0.69565217, 0.        , 0.9044586 ],\n",
              "       [0.10505629, 0.30292735, 0.65217391, 0.45      , 0.86624204],\n",
              "       [0.10629881, 0.30650591, 0.32608696, 0.55      , 0.73673036],\n",
              "       [0.3335531 , 0.41181315, 0.10869565, 0.75      , 0.82165605],\n",
              "       [0.30467086, 0.42531779, 0.2173913 , 0.7       , 0.87685775],\n",
              "       [0.06788214, 0.54538565, 0.41304348, 0.5       , 0.80042463],\n",
              "       [0.12359266, 0.62467169, 0.54347826, 0.55      , 0.53078556],\n",
              "       [0.12803023, 0.53688125, 0.2826087 , 0.7       , 0.49044586],\n",
              "       [0.28230551, 0.59973709, 0.23913043, 0.75      , 0.66878981],\n",
              "       [0.30844913, 0.51315838, 0.41304348, 0.65      , 0.81104034],\n",
              "       [0.11596004, 0.57392069, 0.15217391, 1.        , 0.92144374],\n",
              "       [0.15184096, 0.45383934, 0.15217391, 0.85      , 0.89596603],\n",
              "       [0.03628664, 0.35303856, 0.02173913, 0.8       , 0.74097665],\n",
              "       [0.03770666, 0.39768815, 0.52173913, 0.4       , 0.52229299],\n",
              "       [0.18589614, 0.48931469, 0.17391304, 0.8       , 0.64543524],\n",
              "       [0.34587686, 0.5107819 , 0.26086957, 0.55      , 0.81528662],\n",
              "       [0.41801907, 0.63016699, 0.26086957, 0.6       , 0.69639066],\n",
              "       [0.14230652, 0.78240438, 0.23913043, 0.55      , 0.63906582],\n",
              "       [0.1548585 , 0.70508452, 0.39130435, 0.45      , 0.93630573],\n",
              "       [0.17955675, 0.54952939, 0.45652174, 0.4       , 1.        ],\n",
              "       [0.38830003, 0.49870053, 0.47826087, 0.3       , 0.89808917],\n",
              "       [0.52540826, 0.60495555, 0.60869565, 0.3       , 0.75159236],\n",
              "       [0.26660919, 0.50462786, 0.04347826, 0.9       , 0.67728238],\n",
              "       [0.32500761, 0.49000364, 0.23913043, 0.65      , 0.53078556],\n",
              "       [0.20689218, 0.56131315, 0.02173913, 0.8       , 0.76645435],\n",
              "       [0.21404301, 0.6080019 , 0.06521739, 0.8       , 0.73036093],\n",
              "       [0.33958819, 0.87232741, 0.39130435, 0.7       , 0.77707006],\n",
              "       [0.51790242, 0.53881609, 0.7173913 , 0.1       , 0.80042463],\n",
              "       [0.54310782, 0.57733315, 0.47826087, 0.5       , 0.70063694],\n",
              "       [0.31458566, 0.59780725, 0.52173913, 0.6       , 0.70488323],\n",
              "       [0.3601278 , 0.85365297, 0.30434783, 0.6       , 0.5881104 ],\n",
              "       [0.46607161, 0.88368805, 0.7826087 , 0.4       , 0.38853503],\n",
              "       [0.60721168, 0.78940601, 0.7173913 , 0.45      , 0.45435244],\n",
              "       [0.67631098, 0.4842547 , 0.39130435, 0.6       , 0.43949045],\n",
              "       [0.39048078, 0.47488227, 0.73913043, 0.4       , 0.43099788],\n",
              "       [0.36890151, 0.68755351, 0.5       , 0.4       , 0.74097665],\n",
              "       [0.20532001, 0.88055842, 0.43478261, 0.55      , 0.82802548],\n",
              "       [0.36443858, 0.86920658, 0.82608696, 0.5       , 0.74946921],\n",
              "       [0.51724313, 0.80343476, 0.5       , 0.55      , 0.78343949],\n",
              "       [0.76384522, 0.89059395, 0.41304348, 0.65      , 0.81953291],\n",
              "       [0.72274064, 0.76305244, 0.39130435, 0.4       , 0.77919321],\n",
              "       [0.51807993, 0.50680916, 0.2826087 , 0.55      , 0.83864119],\n",
              "       [0.42664063, 0.44031532, 0.69565217, 0.15      , 0.77070064],\n",
              "       [0.44411198, 0.44847018, 0.34782609, 0.7       , 0.63481953],\n",
              "       [0.5765037 , 0.53194819, 0.43478261, 0.65      , 0.54989384],\n",
              "       [0.62498732, 0.46411415, 0.52173913, 0.5       , 0.38004246],\n",
              "       [0.27720864, 0.43781646, 0.19565217, 0.8       , 0.71762208],\n",
              "       [0.18472969, 0.36321427, 0.26086957, 0.75      , 0.5626327 ],\n",
              "       [0.2157166 , 0.27132054, 0.30434783, 0.5       , 0.66666667],\n",
              "       [0.34630794, 0.41504115, 0.47826087, 0.65      , 0.92781316],\n",
              "       [0.52814687, 0.46424298, 0.58695652, 0.45      , 0.85350318],\n",
              "       [0.69629273, 0.35594407, 0.65217391, 0.45      , 0.80467091],\n",
              "       [0.79348818, 0.23366338, 0.52173913, 0.5       , 0.81953291],\n",
              "       [0.62181763, 0.1942439 , 0.54347826, 0.6       , 0.76008493],\n",
              "       [0.48052541, 0.22368937, 0.52173913, 0.6       , 0.77070064],\n",
              "       [0.47537783, 0.23221541, 0.56521739, 0.7       , 0.64755839],\n",
              "       [0.67357237, 0.22260989, 0.32608696, 0.85      , 0.68577495],\n",
              "       [0.8469926 , 0.25067129, 0.17391304, 0.9       , 0.72186837],\n",
              "       [0.59250431, 0.77265044, 0.56521739, 0.2       , 0.6836518 ],\n",
              "       [0.58690029, 0.37490407, 0.54347826, 0.3       , 0.81953291],\n",
              "       [0.44459377, 0.33594831, 0.58695652, 0.35      , 0.8492569 ],\n",
              "       [0.57424688, 0.78922405, 0.56521739, 0.3       , 0.77494692],\n",
              "       [0.70002029, 0.59263116, 0.80434783, 0.2       , 0.7388535 ],\n",
              "       [0.913759  , 0.02686222, 0.58695652, 0.4       , 0.77919321],\n",
              "       [0.93067248, 0.04547478, 0.36956522, 0.6       , 0.71549894],\n",
              "       [0.60652703, 0.13895982, 0.45652174, 0.55      , 0.63057325],\n",
              "       [0.64730196, 0.23444921, 0.30434783, 0.8       , 0.57324841],\n",
              "       [0.64778375, 0.08766357, 0.02173913, 0.85      , 0.48407643],\n",
              "       [0.85579166, 0.        , 0.19565217, 0.85      , 0.4670913 ],\n",
              "       [0.8702201 , 0.00896779, 0.36956522, 0.75      , 0.50530786],\n",
              "       [0.82832945, 0.05460509, 0.56521739, 0.3       , 0.43949045],\n",
              "       [0.52492646, 0.06346214, 0.45652174, 0.8       , 0.35881104],\n",
              "       [0.56397708, 0.06909052, 0.58695652, 0.4       , 0.39065817],\n",
              "       [0.78456233, 0.17378135, 0.5       , 0.55      , 0.34607219],\n",
              "       [0.89050614, 0.28951841, 0.86956522, 0.25      , 0.4522293 ],\n",
              "       [0.74769246, 0.32281821, 0.32608696, 0.8       , 0.3566879 ],\n",
              "       [0.77834973, 0.28181479, 0.60869565, 0.2       , 0.33970276],\n",
              "       [0.86778578, 0.32219124, 0.7173913 , 0.25      , 0.22929936],\n",
              "       [0.8920783 , 0.40076712, 0.80434783, 0.15      , 0.        ],\n",
              "       [0.65528958, 0.36358884, 0.76086957, 0.3       , 0.04670913],\n",
              "       [0.72162491, 0.34740845, 0.69565217, 0.15      , 0.30360934],\n",
              "       [0.97641749, 0.25359944, 0.32608696, 0.8       , 0.6836518 ],\n",
              "       [0.89631301, 0.29629426, 0.58695652, 0.7       , 0.78343949],\n",
              "       [0.86208033, 0.29488047, 0.56521739, 0.7       , 0.62845011],\n",
              "       [0.82264936, 0.33577854, 0.30434783, 0.8       , 0.37579618],\n",
              "       [0.93237144, 0.33632988, 0.95652174, 0.25      , 0.29511677],\n",
              "       [0.88221422, 0.3205074 , 0.41304348, 0.55      , 0.14225053],\n",
              "       [0.66038645, 0.23447914, 0.54347826, 0.65      , 0.1104034 ],\n",
              "       [0.7679785 , 0.23558352, 0.47826087, 0.65      , 0.22505308],\n",
              "       [0.85898671, 0.27163232, 0.56521739, 0.6       , 0.18471338],\n",
              "       [0.90896643, 0.24948713, 0.60869565, 0.55      , 0.23142251],\n",
              "       [0.80373263, 0.2476273 , 0.73913043, 0.35      , 0.21868365],\n",
              "       [0.87790344, 0.2558132 , 1.        , 0.3       , 0.33121019],\n",
              "       [0.77005782, 0.26404455, 0.41304348, 0.55      , 0.40764331],\n",
              "       [0.83740744, 0.30907516, 0.52173913, 0.55      , 0.45010616],\n",
              "       [0.66008216, 0.31908758, 0.5       , 0.5       , 0.48832272],\n",
              "       [0.76594989, 0.28689914, 0.5       , 0.3       , 0.50955414],\n",
              "       [1.        , 0.30684641, 0.19565217, 1.        , 0.4118896 ],\n",
              "       [0.85670453, 0.31861257, 0.73913043, 0.6       , 0.40764331],\n",
              "       [0.95438178, 0.33193491, 0.2826087 , 0.9       , 0.49681529],\n",
              "       [0.88236637, 0.25044291, 0.63043478, 0.45      , 0.44585987],\n",
              "       [0.86545289, 0.32144657, 0.52173913, 0.7       , 0.38004246],\n",
              "       [0.71259763, 0.34064554, 0.80434783, 0.45      , 0.33545648],\n",
              "       [0.96168476, 0.40479245, 0.63043478, 0.55      , 0.33970276],\n",
              "       [0.75360077, 0.48647925, 0.65217391, 0.25      , 0.53927813],\n",
              "       [0.78897454, 0.56731581, 0.67391304, 0.3       , 0.59023355],\n",
              "       [0.80819556, 0.69027934, 0.26086957, 0.75      , 0.5329087 ],\n",
              "       [0.31179633, 0.52647905, 0.        , 0.9       , 0.51167728],\n",
              "       [0.27553504, 0.36632785, 0.39130435, 0.5       , 0.54140127],\n",
              "       [0.55700375, 0.2112999 , 0.41304348, 0.65      , 0.53078556]])"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scalerKmeans, 'scalerKmeans.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezga_l12F_ms",
        "outputId": "9a6f4e63-76d9-450d-b1b1-c3f28ded054f"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scalerKmeans.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 2, random_state=44)\n",
        "kmeans.fit(x_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOHLEI2q6PYs",
        "outputId": "adf71e04-4108-47ee-b267-2a5c0c017a07"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=2, random_state=44)"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(kmeans, 'kmeans.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpkuQC1eFsrM",
        "outputId": "ad4db251-f4cc-44e0-f41d-7590b6685593"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kmeans.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kmeans.cluster_centers_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2thQvGD66Vey",
        "outputId": "e3635d82-7c8a-4e7d-950b-66ee63f44466"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.29149082 0.5046268  0.42028986 0.52463768 0.74211514]\n",
            " [0.77578953 0.29150608 0.5298913  0.54910714 0.45530027]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJkSje2s6Xew",
        "outputId": "48e1ab31-44d3-482b-b6fa-34b0e4978f5a"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['kluster'] = kmeans.labels_\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jSl84EHC6c_6",
        "outputId": "2a8ea4c4-6dd2-4499-e7db-4f07948790ed"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CO2       PM10  Tavg  RH_avg  Pressure  kluster\n",
              "0    72.076  13.413329  29.6    76.0    1.0286        0\n",
              "1    64.442   9.098366  29.1    76.0    1.0318        0\n",
              "2    64.084   6.363176  26.6    87.0    1.0314        0\n",
              "3    60.842   4.621669  29.1    78.0    1.0262        0\n",
              "4    61.798   5.254629  29.7    76.0    1.0246        0\n",
              "..      ...        ...   ...     ...       ...      ...\n",
              "120  91.658   8.068038  29.4    76.0    1.0198        1\n",
              "121  92.416   9.587104  27.5    85.0    1.0171        1\n",
              "122  72.840   7.563549  26.3    88.0    1.0161        0\n",
              "123  71.410   5.585073  28.1    80.0    1.0175        0\n",
              "124  82.510   3.669890  28.2    83.0    1.0170        1\n",
              "\n",
              "[125 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dfd85eab-2a2f-4bfc-8aec-5fdc9a0fe588\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>Tavg</th>\n",
              "      <th>RH_avg</th>\n",
              "      <th>Pressure</th>\n",
              "      <th>kluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72.076</td>\n",
              "      <td>13.413329</td>\n",
              "      <td>29.6</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0286</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64.442</td>\n",
              "      <td>9.098366</td>\n",
              "      <td>29.1</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0318</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.084</td>\n",
              "      <td>6.363176</td>\n",
              "      <td>26.6</td>\n",
              "      <td>87.0</td>\n",
              "      <td>1.0314</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60.842</td>\n",
              "      <td>4.621669</td>\n",
              "      <td>29.1</td>\n",
              "      <td>78.0</td>\n",
              "      <td>1.0262</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61.798</td>\n",
              "      <td>5.254629</td>\n",
              "      <td>29.7</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0246</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>91.658</td>\n",
              "      <td>8.068038</td>\n",
              "      <td>29.4</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.0198</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>92.416</td>\n",
              "      <td>9.587104</td>\n",
              "      <td>27.5</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1.0171</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>72.840</td>\n",
              "      <td>7.563549</td>\n",
              "      <td>26.3</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1.0161</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>71.410</td>\n",
              "      <td>5.585073</td>\n",
              "      <td>28.1</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0175</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>82.510</td>\n",
              "      <td>3.669890</td>\n",
              "      <td>28.2</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1.0170</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows  6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfd85eab-2a2f-4bfc-8aec-5fdc9a0fe588')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dfd85eab-2a2f-4bfc-8aec-5fdc9a0fe588 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dfd85eab-2a2f-4bfc-8aec-5fdc9a0fe588');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot pada variabel CO2 dan PM10\n",
        "output = plt.scatter(x_scaled[:,0], x_scaled[:,1], s = 100, c = df.kluster, marker = 'o', alpha = 1, )\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:,0], centers[:,1], c='red', s=200, alpha=1 , marker='o');\n",
        "plt.title('Hasil Klustering K-Means')\n",
        "plt.colorbar (output)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "UMHKlZIZ6rxL",
        "outputId": "0a5fe155-342c-4a19-d6bb-4f0122ca4bdb"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('cluster.csv', index=False)"
      ],
      "metadata": {
        "id": "sukiHMEi7Ocy"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dilakukan analisis berdasarkan threshold kebencanaan, dapat diketahui bahwa cluster 0 merupakan kondisi normal, sementara cluster 1 merupakan kondisi bahaya"
      ],
      "metadata": {
        "id": "PmS-5wYo_9mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pengujian"
      ],
      "metadata": {
        "id": "PIefHK8pBPdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Forecast LSTM"
      ],
      "metadata": {
        "id": "uQx5x8G9Ffny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# memasukkan data test\n",
        "df = pd.read_csv('testingbencana.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "b3WBmONACEFN",
        "outputId": "6112ab0d-02c3-4b0b-9bb6-a36dade258ed"
      },
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       CO2      PM10  Tavg  RH_avg  Pressure\n",
              "0   82.785  1.913064  29.0      78    1.0104\n",
              "1   91.484  3.206393  28.6      81    1.0083\n",
              "2   95.662  4.636185  30.3      75    1.0133\n",
              "3   90.030  5.047564  27.8      86    1.0088\n",
              "4   91.239  4.541016  29.1      74    1.0080\n",
              "5   94.766  5.039819  29.6      75    1.0028\n",
              "6   95.724  6.010529  30.0      73    0.9920\n",
              "7   72.559  6.313823  27.3      84    1.0333\n",
              "8   63.221  7.797117  28.2      80    1.0297\n",
              "9   65.418  8.776600  28.8      81    1.0170\n",
              "10  65.593  7.692055  27.6      84    1.0151\n",
              "11  71.677  8.468563  27.4      85    1.0235\n",
              "12  72.708  7.398988  28.2      83    1.0302\n",
              "13  65.117  8.149633  27.0      90    1.0354\n",
              "14  66.532  6.666172  27.0      87    1.0342\n",
              "15  61.975  5.420900  26.4      86    1.0269\n",
              "16  62.031  5.972492  28.7      78    1.0166"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c11b54a5-bfd7-4d45-be98-86cf17a7dd6b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>Tavg</th>\n",
              "      <th>RH_avg</th>\n",
              "      <th>Pressure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82.785</td>\n",
              "      <td>1.913064</td>\n",
              "      <td>29.0</td>\n",
              "      <td>78</td>\n",
              "      <td>1.0104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>91.484</td>\n",
              "      <td>3.206393</td>\n",
              "      <td>28.6</td>\n",
              "      <td>81</td>\n",
              "      <td>1.0083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>95.662</td>\n",
              "      <td>4.636185</td>\n",
              "      <td>30.3</td>\n",
              "      <td>75</td>\n",
              "      <td>1.0133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90.030</td>\n",
              "      <td>5.047564</td>\n",
              "      <td>27.8</td>\n",
              "      <td>86</td>\n",
              "      <td>1.0088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>91.239</td>\n",
              "      <td>4.541016</td>\n",
              "      <td>29.1</td>\n",
              "      <td>74</td>\n",
              "      <td>1.0080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>94.766</td>\n",
              "      <td>5.039819</td>\n",
              "      <td>29.6</td>\n",
              "      <td>75</td>\n",
              "      <td>1.0028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>95.724</td>\n",
              "      <td>6.010529</td>\n",
              "      <td>30.0</td>\n",
              "      <td>73</td>\n",
              "      <td>0.9920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>72.559</td>\n",
              "      <td>6.313823</td>\n",
              "      <td>27.3</td>\n",
              "      <td>84</td>\n",
              "      <td>1.0333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>63.221</td>\n",
              "      <td>7.797117</td>\n",
              "      <td>28.2</td>\n",
              "      <td>80</td>\n",
              "      <td>1.0297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>65.418</td>\n",
              "      <td>8.776600</td>\n",
              "      <td>28.8</td>\n",
              "      <td>81</td>\n",
              "      <td>1.0170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>65.593</td>\n",
              "      <td>7.692055</td>\n",
              "      <td>27.6</td>\n",
              "      <td>84</td>\n",
              "      <td>1.0151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>71.677</td>\n",
              "      <td>8.468563</td>\n",
              "      <td>27.4</td>\n",
              "      <td>85</td>\n",
              "      <td>1.0235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>72.708</td>\n",
              "      <td>7.398988</td>\n",
              "      <td>28.2</td>\n",
              "      <td>83</td>\n",
              "      <td>1.0302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>65.117</td>\n",
              "      <td>8.149633</td>\n",
              "      <td>27.0</td>\n",
              "      <td>90</td>\n",
              "      <td>1.0354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>66.532</td>\n",
              "      <td>6.666172</td>\n",
              "      <td>27.0</td>\n",
              "      <td>87</td>\n",
              "      <td>1.0342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>61.975</td>\n",
              "      <td>5.420900</td>\n",
              "      <td>26.4</td>\n",
              "      <td>86</td>\n",
              "      <td>1.0269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>62.031</td>\n",
              "      <td>5.972492</td>\n",
              "      <td>28.7</td>\n",
              "      <td>78</td>\n",
              "      <td>1.0166</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c11b54a5-bfd7-4d45-be98-86cf17a7dd6b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c11b54a5-bfd7-4d45-be98-86cf17a7dd6b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c11b54a5-bfd7-4d45-be98-86cf17a7dd6b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 404
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelsuhurh = tensorflow.keras.models.load_model('suhurh.h5')\n",
        "modelco2 = tensorflow.keras.models.load_model('co2.h5')\n",
        "modeltekanan = tensorflow.keras.models.load_model('tekanan.h5')\n",
        "modeldebu = tensorflow.keras.models.load_model('debu.h5')"
      ],
      "metadata": {
        "id": "GXggBG87A-QY"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = joblib.load('scalersuhurh.gz')\n",
        "suhurh = scaler.transform(df.iloc[:, 2:4])\n",
        "suhurh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOU75YHjCeJ4",
        "outputId": "bdac8180-e9bb-4fd7-c201-517212c6babe"
      },
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.52439024, 0.64705882],\n",
              "       [0.47560976, 0.70588235],\n",
              "       [0.68292683, 0.58823529],\n",
              "       [0.37804878, 0.80392157],\n",
              "       [0.53658537, 0.56862745],\n",
              "       [0.59756098, 0.58823529],\n",
              "       [0.64634146, 0.54901961],\n",
              "       [0.31707317, 0.76470588],\n",
              "       [0.42682927, 0.68627451],\n",
              "       [0.5       , 0.70588235],\n",
              "       [0.35365854, 0.76470588],\n",
              "       [0.32926829, 0.78431373],\n",
              "       [0.42682927, 0.74509804],\n",
              "       [0.2804878 , 0.88235294],\n",
              "       [0.2804878 , 0.82352941],\n",
              "       [0.20731707, 0.80392157],\n",
              "       [0.48780488, 0.64705882]])"
            ]
          },
          "metadata": {},
          "execution_count": 406
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deret_waktu(data, n_in=1, n_out=1, dropnan=True,):\n",
        "    n_vars = (1 if type(data) is list else data.shape[1])\n",
        "    df = DataFrame(data)\n",
        "    (cols, names) = (list(), list())\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += ['var%d(t-%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += ['var%d(t)' % (j + 1) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += ['var%d(t+%d)' % (j + 1, i) for j in range(n_vars)]\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "zRd7oJ16Cjvn"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag = 2\n",
        "reframedSuhuRh = deret_waktu(suhurh, lag, 0).values\n",
        "reframedCO2 = deret_waktu(df.CO2.values.tolist(), lag, 0).values\n",
        "reframedTekanan = deret_waktu(df.Pressure.values.tolist(), lag, 0).values\n",
        "reframedDebu = deret_waktu(df.PM10.values.tolist(), lag, 0).values"
      ],
      "metadata": {
        "id": "BKdxV9qvD56Y"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframedSuhuRh = reframedSuhuRh.reshape(reframedSuhuRh.shape[0], lag, 2)\n",
        "reframedCO2 = reframedCO2.reshape(reframedCO2.shape[0], lag, 1)\n",
        "reframedTekanan = reframedTekanan.reshape(reframedCO2.shape[0], lag, 1)\n",
        "reframedDebu = reframedDebu.reshape(reframedCO2.shape[0], lag, 1)"
      ],
      "metadata": {
        "id": "eACwurdQKG6r"
      },
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suhurhPred = modelsuhurh.predict(reframedSuhuRh)\n",
        "suhurhPred = scaler.inverse_transform(suhurhPred)\n",
        "\n",
        "co2Pred = modelco2.predict(reframedCO2)\n",
        "\n",
        "tekananPred = modeltekanan.predict(reframedTekanan)\n",
        "\n",
        "debuPred = modeldebu.predict(reframedDebu)"
      ],
      "metadata": {
        "id": "mz-_6I5kEhCa"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfPred = pd.concat([pd.DataFrame(co2Pred), pd.DataFrame(debuPred), pd.DataFrame(suhurhPred), pd.DataFrame(tekananPred)], axis=1)\n",
        "dfPred.set_axis(df.columns, axis=1, inplace=True)\n",
        "dfPred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "IuuPRHLGE8Sh",
        "outputId": "88d6dd83-8db7-409d-ecc9-b74c7b243bbc"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          CO2      PM10       Tavg     RH_avg  Pressure\n",
              "0   86.082321  3.502042  28.642754  80.344704  1.011982\n",
              "1   86.613205  5.045492  29.552738  78.226715  1.011762\n",
              "2   85.591034  5.719075  28.440432  82.512672  1.013718\n",
              "3   85.943481  5.189008  28.892061  77.813156  1.011019\n",
              "4   86.508224  5.705101  29.209677  76.211502  1.009561\n",
              "5   86.604218  6.645941  29.554359  75.226135  1.004532\n",
              "6   71.390083  6.986192  28.255688  80.681709  1.006414\n",
              "7   65.007240  8.449786  28.363041  80.018028  1.029075\n",
              "8   67.943970  9.516565  28.686659  80.551094  1.024579\n",
              "9   67.793648  8.058274  28.201126  81.807076  1.017030\n",
              "10  73.462997  9.204661  28.015167  82.166656  1.017581\n",
              "11  73.561577  7.768992  28.412666  81.612267  1.023629\n",
              "12  66.603668  8.877580  27.926550  83.933258  1.028425\n",
              "13  68.631935  6.881410  27.909832  83.188896  1.031134\n",
              "14  64.565033  5.794801  27.526705  82.429794  1.029041"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cbefc0c-eebe-415a-bb83-3628f7699074\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>Tavg</th>\n",
              "      <th>RH_avg</th>\n",
              "      <th>Pressure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86.082321</td>\n",
              "      <td>3.502042</td>\n",
              "      <td>28.642754</td>\n",
              "      <td>80.344704</td>\n",
              "      <td>1.011982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>86.613205</td>\n",
              "      <td>5.045492</td>\n",
              "      <td>29.552738</td>\n",
              "      <td>78.226715</td>\n",
              "      <td>1.011762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85.591034</td>\n",
              "      <td>5.719075</td>\n",
              "      <td>28.440432</td>\n",
              "      <td>82.512672</td>\n",
              "      <td>1.013718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.943481</td>\n",
              "      <td>5.189008</td>\n",
              "      <td>28.892061</td>\n",
              "      <td>77.813156</td>\n",
              "      <td>1.011019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86.508224</td>\n",
              "      <td>5.705101</td>\n",
              "      <td>29.209677</td>\n",
              "      <td>76.211502</td>\n",
              "      <td>1.009561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>86.604218</td>\n",
              "      <td>6.645941</td>\n",
              "      <td>29.554359</td>\n",
              "      <td>75.226135</td>\n",
              "      <td>1.004532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>71.390083</td>\n",
              "      <td>6.986192</td>\n",
              "      <td>28.255688</td>\n",
              "      <td>80.681709</td>\n",
              "      <td>1.006414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>65.007240</td>\n",
              "      <td>8.449786</td>\n",
              "      <td>28.363041</td>\n",
              "      <td>80.018028</td>\n",
              "      <td>1.029075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>67.943970</td>\n",
              "      <td>9.516565</td>\n",
              "      <td>28.686659</td>\n",
              "      <td>80.551094</td>\n",
              "      <td>1.024579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>67.793648</td>\n",
              "      <td>8.058274</td>\n",
              "      <td>28.201126</td>\n",
              "      <td>81.807076</td>\n",
              "      <td>1.017030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>73.462997</td>\n",
              "      <td>9.204661</td>\n",
              "      <td>28.015167</td>\n",
              "      <td>82.166656</td>\n",
              "      <td>1.017581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>73.561577</td>\n",
              "      <td>7.768992</td>\n",
              "      <td>28.412666</td>\n",
              "      <td>81.612267</td>\n",
              "      <td>1.023629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>66.603668</td>\n",
              "      <td>8.877580</td>\n",
              "      <td>27.926550</td>\n",
              "      <td>83.933258</td>\n",
              "      <td>1.028425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>68.631935</td>\n",
              "      <td>6.881410</td>\n",
              "      <td>27.909832</td>\n",
              "      <td>83.188896</td>\n",
              "      <td>1.031134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64.565033</td>\n",
              "      <td>5.794801</td>\n",
              "      <td>27.526705</td>\n",
              "      <td>82.429794</td>\n",
              "      <td>1.029041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cbefc0c-eebe-415a-bb83-3628f7699074')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cbefc0c-eebe-415a-bb83-3628f7699074 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cbefc0c-eebe-415a-bb83-3628f7699074');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mencari Cluster dengan K-Means"
      ],
      "metadata": {
        "id": "SCjG9wtyFip-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalerKmeans = joblib.load('scalerKmeans.gz')\n",
        "kmeans = joblib.load('kmeans.gz')"
      ],
      "metadata": {
        "id": "UwB80kgoFlG8"
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfPredScaled = scalerKmeans.transform(dfPred)\n",
        "dfPredScaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFzkK36yF32Z",
        "outputId": "fdaea2f9-68da-445d-8052-2007a50bd4ef"
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6475891 , 0.19771318, 0.5092942 , 0.5172353 , 0.4242401 ],\n",
              "       [0.6610509 , 0.32265046, 0.7071168 , 0.4113357 , 0.41957283],\n",
              "       [0.63513124, 0.37717482, 0.46531123, 0.6256337 , 0.46110916],\n",
              "       [0.64406836, 0.33426765, 0.5634915 , 0.3906579 , 0.40380287],\n",
              "       [0.658389  , 0.37604365, 0.6325385 , 0.310575  , 0.37283897],\n",
              "       [0.660823  , 0.45220166, 0.70746964, 0.26130676, 0.2660675 ],\n",
              "       [0.27503002, 0.4797439 , 0.42514962, 0.5340853 , 0.3060341 ],\n",
              "       [0.11317681, 0.5982171 , 0.44848698, 0.5009012 , 0.7871456 ],\n",
              "       [0.18764506, 0.6845695 , 0.51883906, 0.5275545 , 0.6917    ],\n",
              "       [0.18383323, 0.5665255 , 0.4132883 , 0.59035397, 0.53141403],\n",
              "       [0.32759404, 0.65932184, 0.37286252, 0.60833263, 0.54312325],\n",
              "       [0.33009374, 0.543109  , 0.45927542, 0.58061314, 0.67152023],\n",
              "       [0.15365826, 0.63284564, 0.35359782, 0.6966629 , 0.7733574 ],\n",
              "       [0.20509015, 0.47126216, 0.34996337, 0.6594448 , 0.83086777],\n",
              "       [0.10196351, 0.38330463, 0.26667517, 0.6214895 , 0.7864399 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfPredScaled = dfPredScaled.astype(float)\n",
        "clusterPred = kmeans.predict(dfPredScaled)\n",
        "clusterPred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fMHSeVGUZo",
        "outputId": "28982c3a-af0e-4b7f-ee72-6c26a741c020"
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 414
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfPred['Kategori'] = clusterPred\n",
        "dfPred.loc[dfPred['Kategori'] == 1, 'Kategori'] = 'Bahaya'\n",
        "dfPred.loc[dfPred['Kategori'] == 0, 'Kategori'] = 'Normal'\n",
        "dfPred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "AFYE6oOYNT-J",
        "outputId": "6e747cd0-eb93-45fb-97f3-7f0265a40409"
      },
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          CO2      PM10       Tavg     RH_avg  Pressure Kategori\n",
              "0   86.082321  3.502042  28.642754  80.344704  1.011982   Bahaya\n",
              "1   86.613205  5.045492  29.552738  78.226715  1.011762   Bahaya\n",
              "2   85.591034  5.719075  28.440432  82.512672  1.013718   Bahaya\n",
              "3   85.943481  5.189008  28.892061  77.813156  1.011019   Bahaya\n",
              "4   86.508224  5.705101  29.209677  76.211502  1.009561   Bahaya\n",
              "5   86.604218  6.645941  29.554359  75.226135  1.004532   Bahaya\n",
              "6   71.390083  6.986192  28.255688  80.681709  1.006414   Normal\n",
              "7   65.007240  8.449786  28.363041  80.018028  1.029075   Normal\n",
              "8   67.943970  9.516565  28.686659  80.551094  1.024579   Normal\n",
              "9   67.793648  8.058274  28.201126  81.807076  1.017030   Normal\n",
              "10  73.462997  9.204661  28.015167  82.166656  1.017581   Normal\n",
              "11  73.561577  7.768992  28.412666  81.612267  1.023629   Normal\n",
              "12  66.603668  8.877580  27.926550  83.933258  1.028425   Normal\n",
              "13  68.631935  6.881410  27.909832  83.188896  1.031134   Normal\n",
              "14  64.565033  5.794801  27.526705  82.429794  1.029041   Normal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f24be59-ab50-44c3-8067-503fb63a9eb5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>Tavg</th>\n",
              "      <th>RH_avg</th>\n",
              "      <th>Pressure</th>\n",
              "      <th>Kategori</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86.082321</td>\n",
              "      <td>3.502042</td>\n",
              "      <td>28.642754</td>\n",
              "      <td>80.344704</td>\n",
              "      <td>1.011982</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>86.613205</td>\n",
              "      <td>5.045492</td>\n",
              "      <td>29.552738</td>\n",
              "      <td>78.226715</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85.591034</td>\n",
              "      <td>5.719075</td>\n",
              "      <td>28.440432</td>\n",
              "      <td>82.512672</td>\n",
              "      <td>1.013718</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.943481</td>\n",
              "      <td>5.189008</td>\n",
              "      <td>28.892061</td>\n",
              "      <td>77.813156</td>\n",
              "      <td>1.011019</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86.508224</td>\n",
              "      <td>5.705101</td>\n",
              "      <td>29.209677</td>\n",
              "      <td>76.211502</td>\n",
              "      <td>1.009561</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>86.604218</td>\n",
              "      <td>6.645941</td>\n",
              "      <td>29.554359</td>\n",
              "      <td>75.226135</td>\n",
              "      <td>1.004532</td>\n",
              "      <td>Bahaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>71.390083</td>\n",
              "      <td>6.986192</td>\n",
              "      <td>28.255688</td>\n",
              "      <td>80.681709</td>\n",
              "      <td>1.006414</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>65.007240</td>\n",
              "      <td>8.449786</td>\n",
              "      <td>28.363041</td>\n",
              "      <td>80.018028</td>\n",
              "      <td>1.029075</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>67.943970</td>\n",
              "      <td>9.516565</td>\n",
              "      <td>28.686659</td>\n",
              "      <td>80.551094</td>\n",
              "      <td>1.024579</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>67.793648</td>\n",
              "      <td>8.058274</td>\n",
              "      <td>28.201126</td>\n",
              "      <td>81.807076</td>\n",
              "      <td>1.017030</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>73.462997</td>\n",
              "      <td>9.204661</td>\n",
              "      <td>28.015167</td>\n",
              "      <td>82.166656</td>\n",
              "      <td>1.017581</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>73.561577</td>\n",
              "      <td>7.768992</td>\n",
              "      <td>28.412666</td>\n",
              "      <td>81.612267</td>\n",
              "      <td>1.023629</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>66.603668</td>\n",
              "      <td>8.877580</td>\n",
              "      <td>27.926550</td>\n",
              "      <td>83.933258</td>\n",
              "      <td>1.028425</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>68.631935</td>\n",
              "      <td>6.881410</td>\n",
              "      <td>27.909832</td>\n",
              "      <td>83.188896</td>\n",
              "      <td>1.031134</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64.565033</td>\n",
              "      <td>5.794801</td>\n",
              "      <td>27.526705</td>\n",
              "      <td>82.429794</td>\n",
              "      <td>1.029041</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f24be59-ab50-44c3-8067-503fb63a9eb5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f24be59-ab50-44c3-8067-503fb63a9eb5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f24be59-ab50-44c3-8067-503fb63a9eb5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 416
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('file', 'zip', '/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k15mGOKXROoV",
        "outputId": "af2ce856-ec9b-4227-b42e-5b512573d53a"
      },
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/file.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 418
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f4pXYJp6ReYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}